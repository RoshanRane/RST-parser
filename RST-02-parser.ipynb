{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. RST parser playground\n",
    "\n",
    "This RST parser trains classifiers on RST trees from RST treebank. Here you can provide your own custom sentences to see what trees are generated. If you wish to see how it is implemented, scroll down until you reach \"RST Implementation\" part.\n",
    "\n",
    "Major part of the implementation is located in the \"rst.py\" file. It contains the definition of RSTTree class with algorithms for processing lisp expressions and mainting cumulative data. The file also contains function \"create_tree\" which creates a new RSTTree given list of sentences and trained neural models for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from rst import RSTTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load POS and relation dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('data/parser/relation_dicts.pickle', 'rb') as handle:\n",
    "    relation2idx, idx2relation = pickle.load(handle)\n",
    "\n",
    "with open('data/parser/pos2idx.pickle', 'rb') as handle:\n",
    "    pos2idx = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import logging\n",
    "logging.basicConfig(level=logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_SIZE = 100\n",
    "embed_model = gensim.models.Word2Vec.load(\"data/parser/word2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_sentence_embedding(sentence, embed_model):\n",
    "        embeddings = [embed_model[word] for word in sentence if word in embed_model.wv.vocab]\n",
    "        if len(embeddings) == 0:\n",
    "            return None\n",
    "        word_sum = np.zeros(EMBED_SIZE, dtype='float64')\n",
    "        word_count = 0\n",
    "        for word in embeddings:\n",
    "            word_sum += word\n",
    "            word_count += 1\n",
    "        return word_sum / word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.attrs import POS\n",
    "nlp = spacy.load(\"en\")\n",
    "\n",
    "def get_sentence_vector(sentence, embed_model):\n",
    "    embedding = get_sentence_embedding(sentence, embed_model)\n",
    "    if embedding is None:\n",
    "        return None\n",
    "    doc = nlp(\" \".join(sentence))\n",
    "    root = [token for token in doc if token.head == token][0]\n",
    "    return np.r_[len(sentence), (np.arange(POS) == pos2idx[root.pos_]).astype(np.float64), embedding]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(lhs, rhs, embed_model):\n",
    "    if lhs.text is None or rhs.text is None:\n",
    "        return None\n",
    "    lhs_vector = get_sentence_vector(lhs.text, embed_model)\n",
    "    if lhs_vector is None:\n",
    "        return None\n",
    "    rhs_vector = get_sentence_vector(rhs.text, embed_model)\n",
    "    if rhs_vector is None:\n",
    "        return None\n",
    "    return np.r_[lhs_vector, rhs_vector]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/parser/connection_train_set.pickle', 'rb') as handle:\n",
    "    (conn_train_X, conn_train_Y) = pickle.load(handle)\n",
    "\n",
    "with open('data/parser/connection_test_set.pickle', 'rb') as handle:\n",
    "    (conn_test_X, conn_test_Y) = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/parser/relation_train_set.pickle', 'rb') as handle:\n",
    "    (rel_train_X, rel_train_Y) = pickle.load(handle)\n",
    "\n",
    "with open('data/parser/relation_test_set.pickle', 'rb') as handle:\n",
    "    (rel_test_X, rel_test_Y) = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/parser/nuclearity_train_set.pickle', 'rb') as handle:\n",
    "    (nuc_train_X, nuc_train_Y) = pickle.load(handle)\n",
    "\n",
    "with open('data/parser/nuclearity_test_set.pickle', 'rb') as handle:\n",
    "    (nuc_test_X, nuc_test_Y) = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load NN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_model = load_model(\"data/parser/connection_model.h5\")\n",
    "relation_model = load_model(\"data/parser/relation_model.h5\")\n",
    "nuclearity_model = load_model(\"data/parser/nuclearity_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rst import create_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vpraid/anaconda3/envs/advanced-nlp/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( root (span 0 2) \n",
      "  ( nucleus (span 0 1) (rel2par span)   \n",
      "    ( nucleus (leaf 0) (rel2par span) (text spencer j . volk , president and chief operating officer of this consumer and industrial products company , was elected a director .) )  \n",
      "    ( satellite (leaf 1) (rel2par none) (text blahblah) )  \n",
      ")\n",
      "  ( satellite (leaf 2) (rel2par elaboration-object-attribute-e) (text who retired in september .) )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "sent1 = \"Spencer J. Volk, president and chief operating officer of this consumer and industrial products company, was elected a director.\"\n",
    "sent2 = \"blahblah\"\n",
    "sent3 = \"who retired in September.\"\n",
    "sentence_list = [sent1, sent2, sent3]\n",
    "result = create_tree(\n",
    "    sentence_list,\n",
    "    lambda lhs, rhs: get_vector(lhs, rhs, embed_model),\n",
    "    idx2relation,\n",
    "    connection_model,\n",
    "    relation_model,\n",
    "    nuclearity_model)\n",
    "print(result.output_lisp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. RST implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rst import RSTTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Read the corpus into the list of RSTTree structures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to get a list of trees to navigate the sentences and to later generate them from NN predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class CorpusReader:\n",
    "\n",
    "    def __init__(self, rst_root):\n",
    "        self.rst_root = rst_root\n",
    "    \n",
    "    def load_test_trees(self):\n",
    "        return self._load_trees(\"TEST\")\n",
    "    \n",
    "    def load_train_trees(self):\n",
    "        return self._load_trees(\"TRAINING\")\n",
    "    \n",
    "    def _load_trees(self, dirsuffix):\n",
    "        root_with_suffix = os.path.join(self.rst_root, dirsuffix)\n",
    "        for dirname in os.listdir(root_with_suffix):\n",
    "            dirname = os.path.join(root_with_suffix, dirname)\n",
    "            if os.path.isdir(dirname):\n",
    "                for filename in os.listdir(dirname):\n",
    "                    filename = os.path.join(dirname, filename)\n",
    "                    if os.path.isfile(filename) and len(filename) > 9 and filename[-9:] == \"lisp.name\":\n",
    "                        with open(filename, encoding=\"utf-8\") as file:\n",
    "                            try:\n",
    "                                contents = file.read()\n",
    "                                tree = RSTTree.from_sexp(contents, filename)\n",
    "                                tree.construct_text()\n",
    "                                yield tree\n",
    "                            except AssertionError as err:\n",
    "                                print(\"Error in \", filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_reader = CorpusReader(\"/Users/vpraid/Downloads/RSTDT/data/RSTtrees-WSJ-main-1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Use gensim to transform sentences into word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create EDU reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EduReader:\n",
    "    \n",
    "    def __init__(self, reader):\n",
    "        self.reader = reader\n",
    "\n",
    "    def __iter__(self):\n",
    "        for tree in self.reader.load_train_trees():\n",
    "            for edu in tree.all_edus():\n",
    "                yield edu\n",
    "\n",
    "edu_reader = EduReader(corpus_reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load gensim and create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import logging\n",
    "logging.basicConfig(level=logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_SIZE = 100\n",
    "embed_model = gensim.models.Word2Vec(edu_reader, size=EMBED_SIZE, min_count=2, window=5, iter=100)\n",
    "embed_model.save(\"data/parser/word2vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result embedding shape: (6938, 100)\n",
      "Checking similar words:\n",
      "  money -> liability (0.39), manville (0.39), choice (0.38), developers (0.36)\n",
      "  bank -> montreal (0.51), assurance (0.46), imperial (0.46), equaling (0.40)\n",
      "  company -> transaction (0.40), manville (0.33), group (0.33), restructuring (0.32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vpraid/anaconda3/envs/advanced-nlp/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
      "  \n",
      "/Users/vpraid/anaconda3/envs/advanced-nlp/lib/python3.6/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "def check_similar(model):\n",
    "    pretrained_weights = model.wv.syn0\n",
    "    vocab_size, emdedding_size = pretrained_weights.shape\n",
    "    print('Result embedding shape:', pretrained_weights.shape)\n",
    "    print('Checking similar words:')\n",
    "    for word in ['money', 'bank', 'company']:\n",
    "        most_similar = ', '.join('%s (%.2f)' % (similar, dist) for similar, dist in model.most_similar(word)[:4])\n",
    "        print('  %s -> %s' % (word, most_similar))\n",
    "\n",
    "check_similar(embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embedding(sentence, embed_model):\n",
    "        embeddings = [embed_model[word] for word in sentence if word in embed_model.wv.vocab]\n",
    "        if len(embeddings) == 0:\n",
    "            return None\n",
    "        word_sum = np.zeros(EMBED_SIZE, dtype='float64')\n",
    "        word_count = 0\n",
    "        for word in embeddings:\n",
    "            word_sum += word\n",
    "            word_count += 1\n",
    "        return word_sum / word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Get POS tags from spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.attrs import POS\n",
    "\n",
    "nlp = spacy.load(\"en\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos2idx = {}\n",
    "def fill_pos_tags():\n",
    "    for edu in edu_reader:\n",
    "        doc = nlp(\" \".join(edu))\n",
    "        for token in doc:\n",
    "            if token.pos_ not in pos2idx:\n",
    "                pos2idx[token.pos_] = len(pos2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "fill_pos_tags()\n",
    "with open('data/parser/pos2idx.pickle', 'wb') as handle:\n",
    "    pickle.dump(pos2idx, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_vector(sentence, embed_model):\n",
    "    embedding = get_sentence_embedding(sentence, embed_model)\n",
    "    if embedding is None:\n",
    "        return None\n",
    "    doc = nlp(\" \".join(sentence))\n",
    "    root = [token for token in doc if token.head == token][0]\n",
    "    return np.r_[len(sentence), (np.arange(POS) == pos2idx[root.pos_]).astype(np.float64), embedding]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Train connection classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from tqdm import tqdm_notebook, tnrange\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "tqdm.monitor_interval = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(lhs, rhs, embed_model):\n",
    "    if lhs.text is None or rhs.text is None:\n",
    "        return None\n",
    "    lhs_vector = get_sentence_vector(lhs.text, embed_model)\n",
    "    if lhs_vector is None:\n",
    "        return None\n",
    "    rhs_vector = get_sentence_vector(rhs.text, embed_model)\n",
    "    if rhs_vector is None:\n",
    "        return None\n",
    "    return np.r_[lhs_vector, rhs_vector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_connected(lhs, rhs):\n",
    "    if lhs.parent != rhs.parent:\n",
    "        return False\n",
    "    if lhs.parent.nuclearity == RSTTree.MONONUCLEAR:\n",
    "        return True\n",
    "    assert lhs.type == 'nucleus' and rhs.type == 'nucleus'\n",
    "    return np.abs(lhs.index - rhs.index) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from random import shuffle\n",
    "\n",
    "def shuffled(x):\n",
    "    y = x[:]\n",
    "    shuffle(y)\n",
    "    return y\n",
    "\n",
    "def get_connection_set(trees):\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    for tree in tqdm_notebook(trees):\n",
    "        \n",
    "        subtrees = tree.all_trees()\n",
    "        for subtree in subtrees:\n",
    "            if subtree.nuclearity == RSTTree.MONONUCLEAR:\n",
    "                pair = get_vector(subtree.nuclei[0], subtree.satellite, embed_model)\n",
    "                if pair is None:\n",
    "                    continue\n",
    "                pairs.append(pair)\n",
    "                labels.append(1)\n",
    "            else:\n",
    "                for lhs, rhs in zip(subtree.nuclei, subtree.nuclei[1:]):\n",
    "                    pair = get_vector(lhs, rhs, embed_model)\n",
    "                    if pair is None:\n",
    "                        continue\n",
    "                    pairs.append(pair)\n",
    "                    labels.append(1)\n",
    "\n",
    "        for i, (left, right) in enumerate(product(shuffled(subtrees), shuffled(subtrees))):\n",
    "            if left == right or are_connected(left, right):\n",
    "                continue\n",
    "            if i > 30:\n",
    "                break\n",
    "            pair = get_vector(left, right, embed_model)\n",
    "            if pair is None:\n",
    "                continue\n",
    "            pairs.append(pair)\n",
    "            labels.append(0)\n",
    "\n",
    "    shape = len(pairs), pairs[0].shape[0]\n",
    "    return np.concatenate(pairs).reshape(shape), np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare and save training connection data\n",
    "Do not run this cell unless you are ready to wait 15 minutes for it to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c758769d4e84b1dac75c788373564fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vpraid/anaconda3/envs/advanced-nlp/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "conn_train_X, conn_train_Y = get_connection_set(corpus_reader.load_train_trees())\n",
    "with open('data/parser/connection_train_set.pickle', 'wb') as handle:\n",
    "    pickle.dump((conn_train_X, conn_train_Y), handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare and save test connection data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9af8ffdd858474f95a0d07ff9db1fc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vpraid/anaconda3/envs/advanced-nlp/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "conn_test_X, conn_test_Y = get_connection_set(corpus_reader.load_test_trees())\n",
    "with open('data/parser/connection_test_set.pickle', 'wb') as handle:\n",
    "    pickle.dump((conn_test_X, conn_test_Y), handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import regularizers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_model = Sequential()\n",
    "connection_model.add(Dense(256, input_dim=conn_train_X.shape[1], activation='relu'))\n",
    "connection_model.add(Dropout(0.5))\n",
    "connection_model.add(Dense(128, activation='relu'))\n",
    "connection_model.add(Dropout(0.5))\n",
    "connection_model.add(Dense(64, activation='relu'))\n",
    "connection_model.add(Dropout(0.5))\n",
    "connection_model.add(Dense(1, activation='sigmoid'))\n",
    "connection_model.compile(loss='binary_crossentropy', optimizer=\"adam\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_model.fit(conn_train_X, conn_train_Y, batch_size=128, verbose=0, epochs=15)\n",
    "connection_model.save(\"data/parser/connection_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.32437039797961892, 0.8529528164988347]\n"
     ]
    }
   ],
   "source": [
    "score = connection_model.evaluate(conn_train_X, conn_train_Y, verbose=0)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.76390226314996035, 0.74160305343511446]\n"
     ]
    }
   ],
   "source": [
    "score = connection_model.evaluate(conn_test_X, conn_test_Y, verbose=0)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Train relation classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "def get_relation_set(trees, populate_relations = False, relation2idx = dict()):\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    for tree in tqdm_notebook(trees):\n",
    "        \n",
    "        subtrees = tree.all_trees()\n",
    "        for subtree in subtrees:\n",
    "            \n",
    "            if subtree.relation is None:\n",
    "                continue\n",
    "            \n",
    "            if populate_relations and subtree.relation not in relation2idx:\n",
    "                relation2idx[subtree.relation] = len(relation2idx)\n",
    "            \n",
    "            elif not populate_relations and subtree.relation not in relation2idx.keys():\n",
    "                continue\n",
    "            \n",
    "            if subtree.nuclearity == RSTTree.MONONUCLEAR:\n",
    "                pair = get_vector(subtree.nuclei[0], subtree.satellite, embed_model)\n",
    "                if pair is None:\n",
    "                    continue\n",
    "                pairs.append(pair)\n",
    "                labels.append(relation2idx[subtree.relation])\n",
    "            \n",
    "            else:\n",
    "                for lhs, rhs in zip(subtree.nuclei, subtree.nuclei[1:]):\n",
    "                    pair = get_vector(lhs, rhs, embed_model)\n",
    "                    if pair is None:\n",
    "                        continue\n",
    "                    pairs.append(pair)\n",
    "                    labels.append(relation2idx[subtree.relation])\n",
    "\n",
    "    shape = len(pairs), pairs[0].shape[0]\n",
    "    return np.concatenate(pairs).reshape(shape), to_categorical(labels, num_classes=len(relation2idx)), relation2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training relation data\n",
    "Do not run this cell unless you are ready to wait 10 minutes for it to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43d8d22239914b308ad632138ad5b6be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vpraid/anaconda3/envs/advanced-nlp/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rel_train_X, rel_train_Y, relation2idx = get_relation_set(corpus_reader.load_train_trees(), True)\n",
    "idx2relation = { index : relation for relation, index in relation2idx.items() }\n",
    "with open('data/parser/relation_train_set.pickle', 'wb') as handle:\n",
    "    pickle.dump((rel_train_X, rel_train_Y), handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare test relation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a826c98eebc14148ab003292a512c4e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vpraid/anaconda3/envs/advanced-nlp/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rel_test_X, rel_test_Y, _ = get_relation_set(\n",
    "    corpus_reader.load_test_trees(),\n",
    "    populate_relations=False,\n",
    "    relation2idx=relation2idx)\n",
    "with open('data/parser/relation_test_set.pickle', 'wb') as handle:\n",
    "    pickle.dump((rel_test_X, rel_test_Y), handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save relation dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/parser/relation_dicts.pickle', 'wb') as handle:\n",
    "    pickle.dump((relation2idx, idx2relation), handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_model = Sequential()\n",
    "relation_model.add(Dense(256, input_dim=rel_train_X.shape[1], activation='relu'))\n",
    "relation_model.add(Dropout(0.5))\n",
    "relation_model.add(Dense(128, activation='relu'))\n",
    "relation_model.add(Dropout(0.5))\n",
    "relation_model.add(Dense(64, activation='relu'))\n",
    "relation_model.add(Dropout(0.5))\n",
    "relation_model.add(Dense(rel_train_Y.shape[1], activation='softmax'))\n",
    "relation_model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_model.fit(rel_train_X, rel_train_Y, batch_size=128, verbose=0, epochs=50)\n",
    "relation_model.save(\"data/parser/relation_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.8957532081638413, 0.50907709837593695]\n"
     ]
    }
   ],
   "source": [
    "score = relation_model.evaluate(rel_train_X, rel_train_Y, verbose=0)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.6408729917430755, 0.37676508340205095]\n"
     ]
    }
   ],
   "source": [
    "score = relation_model.evaluate(rel_test_X, rel_test_Y, verbose=0)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Train nuclearity classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUCLEUS_L = [1, 0]\n",
    "NUCLEUS_R = [0, 1]\n",
    "NUCLEUS_B = [1, 1]\n",
    "\n",
    "def get_nuclearity_set(trees):\n",
    "    pairs = []\n",
    "    labels = []\n",
    "    for tree in tqdm_notebook(trees):\n",
    "        \n",
    "        subtrees = tree.all_trees()\n",
    "        for subtree in subtrees:\n",
    "            \n",
    "            if subtree.nuclearity == RSTTree.MONONUCLEAR:\n",
    "                pair = get_vector(subtree.nuclei[0], subtree.satellite, embed_model)\n",
    "                if pair is None:\n",
    "                    continue\n",
    "                pairs.append(pair)\n",
    "                label = NUCLEUS_L if subtree.nuclei[0].span[1] <= subtree.satellite.span[0] else NUCLEUS_R\n",
    "                labels.append(label)\n",
    "            \n",
    "            else:\n",
    "                for lhs, rhs in zip(subtree.nuclei, subtree.nuclei[1:]):\n",
    "                    pair = get_vector(lhs, rhs, embed_model)\n",
    "                    if pair is None:\n",
    "                        continue\n",
    "                    pairs.append(pair)\n",
    "                    labels.append(NUCLEUS_B)\n",
    "\n",
    "    shape = len(pairs), pairs[0].shape[0]\n",
    "    return np.concatenate(pairs).reshape(shape), np.array(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare and save nuclearity relation data\n",
    "Do not run this cell unless you are ready to wait 10 minutes for it to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60b9d5cf111041a4b1abd7568c3e8abb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vpraid/anaconda3/envs/advanced-nlp/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nuc_train_X, nuc_train_Y = get_nuclearity_set(corpus_reader.load_train_trees())\n",
    "with open('data/parser/nuclearity_train_set.pickle', 'wb') as handle:\n",
    "    pickle.dump((nuc_train_X, nuc_train_Y), handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare and save nuclearity test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bc9bc97646d4da7977e69b7690da2b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vpraid/anaconda3/envs/advanced-nlp/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "nuc_test_X, nuc_test_Y = get_nuclearity_set(corpus_reader.load_test_trees())\n",
    "with open('data/parser/nuclearity_test_set.pickle', 'wb') as handle:\n",
    "    pickle.dump((nuc_test_X, nuc_test_Y), handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and train the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuclearity_model = Sequential()\n",
    "nuclearity_model.add(Dense(256, input_dim=nuc_train_X.shape[1], activation='relu'))\n",
    "nuclearity_model.add(Dropout(0.5))\n",
    "nuclearity_model.add(Dense(128, activation='relu'))\n",
    "nuclearity_model.add(Dropout(0.5))\n",
    "nuclearity_model.add(Dense(64, activation='relu'))\n",
    "nuclearity_model.add(Dropout(0.5))\n",
    "nuclearity_model.add(Dense(nuc_train_Y.shape[1], activation='softmax'))\n",
    "nuclearity_model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuclearity_model.fit(nuc_train_X, nuc_train_Y, batch_size=128, verbose=0, epochs=10)\n",
    "nuclearity_model.save(\"data/parser/nuclearity_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5867015601630633, 0.92616786935055073]\n"
     ]
    }
   ],
   "source": [
    "score = nuclearity_model.evaluate(nuc_train_X, nuc_train_Y, verbose=0)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.62819885321152513, 0.90384615354048903]\n"
     ]
    }
   ],
   "source": [
    "score = nuclearity_model.evaluate(nuc_test_X, nuc_test_Y, verbose=0)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def read_edus(dirname):\n",
    "    for filename in os.listdir(dirname):\n",
    "        filename = os.path.join(dirname, filename)\n",
    "        if os.path.isfile(filename) and len(filename) > 9 and filename[-8:] == \"out.edus\":\n",
    "            with open(filename, encoding=\"utf-8\") as file:\n",
    "                yield os.path.basename(filename)[:-9], file.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61783bfcb19849888924278fcfdae6a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/vpraid/anaconda3/envs/advanced-nlp/lib/python3.6/site-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from rst import create_tree\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "for filename, edus in tqdm_notebook(read_edus(\"data/segmentor/results\")):    \n",
    "    result = create_tree(\n",
    "        edus,\n",
    "        lambda lhs, rhs: get_vector(lhs, rhs, embed_model),\n",
    "        idx2relation,\n",
    "        connection_model,\n",
    "        relation_model,\n",
    "        nuclearity_model)\n",
    "    \n",
    "    with open(os.path.join(\"data/parser/results\", filename + \".out.lisp.name\"), \"w\") as file:\n",
    "        file.write(result.output_lisp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
