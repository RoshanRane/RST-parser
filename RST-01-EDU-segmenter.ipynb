{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Library imports\n",
    "import os.path\n",
    "import glob\n",
    "import nltk\n",
    "import gensim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "import pickle\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#constants\n",
    "N_GRAM = 4\n",
    "W2V_SIZE = 100\n",
    "PATH = \"data/segmentor/\"\n",
    "W2V_FILENAME = PATH + \"word2vec-RSTCorpus\"\n",
    "B_ARRAY = np.array([1,0],dtype=np.float64) # The Neural network output expected for EDU Break class\n",
    "C_ARRAY = np.array([0,1],dtype=np.float64) # The Neural network output expected for EDU Continue class\n",
    "NN_MODEL_FILES = PATH + \"nnModel_\" #file name prefixes for storing trained neural net models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Reads the RST-DT corpus data and\n",
    "Returns all sentences tokenized to words and the expected EDU breaks for each of these sentences\n",
    "Attention : The internal folder structure of the corpus must not be altered. \n",
    "'''\n",
    "def readCorpus(relDir, dataClass):    \n",
    "    #go to the corpus dir  \n",
    "    if((dataClass.upper() != \"TRAINING\") and (dataClass.upper() != \"TEST\")):\n",
    "        raise ValueError(\"Value of 'dataClass' is incorrect. Select one of the following - 1)'TRAINING' 2)'TEST'\")\n",
    "    else:\n",
    "        relDir +=  \"/data/RSTtrees-WSJ-main-1.0/\" + dataClass.upper() + \"/\"\n",
    "        fileext = \"*.edus\"        \n",
    " \n",
    "    absDir = os.path.join(os.getcwd(), relDir)\n",
    "    \n",
    "    if(os.path.isdir(absDir)):\n",
    "        print(\"Reading corpus from \"+ absDir)\n",
    "    else:\n",
    "        raise ValueError(\"The dir \" + absDir +\" is incorrect or doesnot exist. Please check check the value set in 'relDir' \")\n",
    "            \n",
    "    files = os.path.join(absDir, fileext)\n",
    "    \n",
    "    tokens = []\n",
    "    edu_idx = []\n",
    "    for fname in sorted(glob.glob(files)):\n",
    "        with open(fname, 'r', encoding='utf-8') as doc:\n",
    "            sent=[]\n",
    "            edu_boundary = []\n",
    "            for edu in doc:\n",
    "                # tokenize to words\n",
    "                edu_tokens = nltk.word_tokenize(edu)\n",
    "                \n",
    "                if edu_tokens[-1] not in [\"!\", \"?\", \".\", \"...\"]:\n",
    "                    # join EDUs of a sentence\n",
    "                    sent.extend(edu_tokens)\n",
    "                    # remember EDU boundary indices\n",
    "                    edu_boundary.append(len(sent) - 1)\n",
    "                else:\n",
    "                    sent.extend(edu_tokens)\n",
    "                    tokens.append(sent)\n",
    "                    edu_idx.append(edu_boundary)\n",
    "                    # clear for next sentence\n",
    "                    sent = []\n",
    "                    edu_boundary = []\n",
    "            \n",
    "    return tokens,edu_idx\n",
    "\n",
    "############################################################################################\n",
    "# below generation code is commented out as it requires the RST Corpus folder to execute\n",
    "\n",
    "\n",
    "# tokenized_train_data,train_EDUs = readCorpus(\"../../RST_corpus\", \"Training\")\n",
    "# tokenized_test_data,test_EDUs = readCorpus(\"../../RST_corpus\", \"Test\")\n",
    "\n",
    "# f_tokenized_train_data = open( \"data/segmentor/tokenized_train_data.p\", \"wb\" )\n",
    "# f_tokenized_test_data = open( \"data/segmentor/tokenized_test_data.p\", \"wb\" )\n",
    "# f_train_EDUs = open( \"data/segmentor/train_EDUs.p\", \"wb\" )\n",
    "# f_test_EDUs = open( \"data/segmentor/test_EDUs.p\", \"wb\" )\n",
    "\n",
    "# pickle.dump(tokenized_train_data, f_tokenized_train_data)\n",
    "# pickle.dump(tokenized_test_data, f_tokenized_test_data)\n",
    "# pickle.dump(train_EDUs, f_train_EDUs)\n",
    "# pickle.dump(test_EDUs, f_test_EDUs)\n",
    "\n",
    "# f_tokenized_test_data.close()\n",
    "# f_tokenized_test_data.close()\n",
    "# f_train_EDUs.close()\n",
    "# f_test_EDUs.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read pre-generated tokens from the RST corpus\n",
    "with open( PATH+\"tokenized_train_data.p\", \"rb\" ) as file:\n",
    "    tokenized_train_data = pickle.load(file)\n",
    "\n",
    "with open( PATH+\"tokenized_test_data.p\", \"rb\" ) as file:\n",
    "    tokenized_test_data = pickle.load(file)\n",
    "\n",
    "with open( PATH+\"train_EDUs.p\", \"rb\" ) as file:\n",
    "    train_EDUs = pickle.load(file)\n",
    "    \n",
    "with open( PATH+\"test_EDUs.p\", \"rb\" ) as file:\n",
    "    test_EDUs = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Word2Vec "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "**Note: Donot Execute the below cell if not necessary **\n",
    "<br>The cell takes approx. **16 minutes** for execution.<br>\n",
    "If the word2vecs are regenerated then **the whole neural network training will have to be repeated** which took **14 hours** on my CPU\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''Plot the Word vectors generated in 2D for visualization '''\n",
    "def w2v_visualizer(model):\n",
    "    labels = []\n",
    "    tokens = []\n",
    "    for word in model.wv.vocab:\n",
    "        tokens.append(model[word])\n",
    "        labels.append(word)\n",
    "    # use the TSNE model from scikit to reduce the dimensions from 1000s to 2\n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "    \n",
    "    #extract the x and y axis and plot a scatter graph\n",
    "    x = [value[0] for value in new_values]\n",
    "    y = [value[1] for value in new_values] \n",
    "    plt.figure(figsize=(20, 16)) \n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i], xy=(x[i], y[i]), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom')\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "'''Generate Word2Vec for all words in the corpus'''\n",
    "def gen_w2v(tokenized_data, veclen, filename, visualize = False):\n",
    "    # Word2Vec - Convert each word token to a vector of size 'veclen'\n",
    "    model = gensim.models.Word2Vec(tokenized_data, size= veclen , window=20, min_count=1)\n",
    "    # Save model as a file\n",
    "    file = model.save(filename)\n",
    "    if(visualize):\n",
    "        w2v_visualizer(model)\n",
    "    #delete the model to save RAM space and return the file\n",
    "    del model\n",
    "\n",
    "#########################################################################################################################\n",
    "\n",
    "gen_w2v(tokenized_train_data + tokenized_test_data, W2V_SIZE, W2V_FILENAME, visualize = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Vectors for the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Takes Corpus extractions and EDU boundaries and converts it to input vectors and \n",
    "output vectors to feed to the Neural Network.\n",
    "Concatenates the words in n-grams and stitches the Word Embeddings and feature vectors of each word.\n",
    "Output is a 2-element vector : [1,0] meaning EDU Break\n",
    "                             : [0,1] meaning no EDU Break '''\n",
    "def extract_nnVec(tokenized_data, edu_lists, model, features_df, n_gram):\n",
    "    \n",
    "    nnData = []\n",
    "    features_len = len(features_df.columns) - 1\n",
    "    total_words = 0\n",
    "    \n",
    "    for sent,EDUs in zip(tokenized_data, edu_lists):        \n",
    "        # Extend the sentences on both ends with 'empty space' tokens. This will ensure that no words are missed.\n",
    "        # One n-gram sequence will exist for every word pair in the original sentence, with the word pair as it's he mid point in it.\n",
    "        ext = [\" \" for i in range((n_gram//2) -1)]\n",
    "        index_offset = len(ext)\n",
    "        extended_sent = ext + sent + ext        \n",
    "        \n",
    "        # get word2Vecs + featureVecs of each word of the n-gram and stack them together\n",
    "        for i in range(len(extended_sent)- n_gram + 1):\n",
    "            \n",
    "            word = extended_sent[i]\n",
    "            #if the word is a filler 'empty space' token then return 0 vectors\n",
    "            if(word == \" \"):\n",
    "                nnDat_in = np.zeros(model.vector_size + features_len, dtype=np.float64)\n",
    "            else:\n",
    "                nnDat_in = np.concatenate((model.wv[word], getFeatureVecs(features_df, total_words+i-index_offset, word )))                \n",
    "                \n",
    "            #collect the next (n-1) words of the n-gram\n",
    "            for j in range(1,n_gram) :\n",
    "                \n",
    "                word = extended_sent[i+j]\n",
    "                \n",
    "                if(word == \" \"):\n",
    "                    nnDat_in = np.concatenate((nnDat_in, np.zeros(model.vector_size + features_len, dtype=np.float64)))\n",
    "                else:\n",
    "                    nnDat_in = np.concatenate((nnDat_in, model.wv[word], getFeatureVecs(features_df, total_words+i-index_offset+j, word)))\n",
    "                    \n",
    "            #check if there is an EDU break in between the n-gram.\n",
    "            mid_index = (i - index_offset + n_gram//2 - 1)\n",
    "            \n",
    "            if(mid_index in EDUs):\n",
    "                nnDat_out = B_ARRAY # EDU 'Break' class\n",
    "            else:\n",
    "                nnDat_out = C_ARRAY # EDU 'Continue' class\n",
    "                \n",
    "            nnData.append((nnDat_in, nnDat_out))\n",
    "            \n",
    "        # increment the total words index by the number of words that were in the sent\n",
    "        total_words += len(sent)  \n",
    "        \n",
    "    return nnData\n",
    "\n",
    "\n",
    "'''Reads a csv file containing pre-generated features that are converted to vectors and extracts vectors in neccessary formats and'''\n",
    "def extractFeatures(csvFile):\n",
    "    df = pd.read_csv(csvFile)\n",
    "    #delete unneccesary columns. Removing top_syntactic tag features\n",
    "    delCols = ['file_name','sent_word_indices']\n",
    "    df = df.drop(delCols,axis = 1)\n",
    "    return df\n",
    "\n",
    "\n",
    "''' Reads the features from the dataframe, checks if the word is correct and\n",
    "returns feature vectors as an np array'''\n",
    "def getFeatureVecs(df, idx, word):\n",
    "    csvword = df.iloc[idx].loc['word']\n",
    "    # Ensure the features are read for the expected word \n",
    "    assert(csvword in [word,'OPENING_ROUND_BRACE', 'CLOSING_ROUND_BRACE']),\\\n",
    "    \"The word in the csv '{}' is not matching with the word '{}' at the index {}\".format(csvword, word, idx)\n",
    "    return np.array(df.iloc[idx].iloc[1:],dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load word vectors and Features previously generated for the test corpus \n",
    "w2v_model = gensim.models.Word2Vec.load(W2V_FILENAME)\n",
    "features_test_df = extractFeatures(\"data_set_1__test.csv\")\n",
    "\n",
    "#generate Vectors for test data\n",
    "testVecs = extract_nnVec(tokenized_test_data, test_EDUs, w2v_model, features_test_df, n_gram = N_GRAM)\n",
    "\n",
    "nn_in_dem = testVecs[0][0].shape[0]\n",
    "nn_out_dem = 2\n",
    "del w2v_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "**Note: Donot Execute the below cell if not necessary **\n",
    "<br>The cell takes approx. **8mins** and large RAM space for execution.<br>\n",
    "The results can still be viewed without generating the training vectors as a pre-trained model is provided.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load word vectors and Features previously generated for the training corpus \n",
    "w2v_model = gensim.models.Word2Vec.load(W2V_FILENAME)\n",
    "features_train_df = extractFeatures(\"data_set_1__train.csv\")\n",
    "\n",
    "#generate Vectors for training data\n",
    "trainingVecs = extract_nnVec(tokenized_train_data, train_EDUs, w2v_model, features_train_df, n_gram = N_GRAM)\n",
    "del w2v_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''Evaluates the neural net model on the test data and calculates the Precision, Recall and F1 scores.\n",
    "Returns the F1-score '''\n",
    "def evaluateModel(nnModel, testVecs, threshold = 0, printDebugs = False):\n",
    "    # calculate the True Positives, False Positives, True Negatives and False Negatives\n",
    "    TP = FP = TN = FN = 0\n",
    "    predictions = []\n",
    "\n",
    "    for in_vec, expected_vec in testVecs:\n",
    "\n",
    "        # Get the predictions of the neural network model and \n",
    "        predicted_vec = nnModel.predict(in_vec).reshape(-1)\n",
    "        \n",
    "        # Check if the Neural Network, is predicting a EDU 'Break' or a EDU 'Continue'\n",
    "        if((predicted_vec[0] - predicted_vec[1]) > threshold ):\n",
    "            predicted = \"B\"\n",
    "        else:\n",
    "            predicted = \"C\"\n",
    "        predictions.append(predicted)\n",
    "        \n",
    "        if(np.array_equal(expected_vec, B_ARRAY)):\n",
    "            expected = 'B'\n",
    "        else:\n",
    "            expected = 'C'      \n",
    "            \n",
    "        if((expected == 'B') and (predicted == 'B')):\n",
    "            TP += 1\n",
    "        elif((expected == 'B') and (predicted == 'C')):\n",
    "            FN += 1\n",
    "        elif((expected == 'C') and (predicted == 'B')):\n",
    "            FP += 1\n",
    "        elif((expected == 'C') and (predicted == 'C')):\n",
    "            TN += 1\n",
    "\n",
    "    if(printDebugs): \n",
    "        print(\"TP {},FN {},FP {},TN {}\".format(TP,FN,FP,TN))\n",
    "\n",
    "    #recall, Precision and F1-score calculation\n",
    "    recall = TP/(TP+FN)\n",
    "    precision = TP/(TP+FP)\n",
    "    F1 = 100*2*recall*precision/(recall + precision)      \n",
    "\n",
    "    if(printDebugs): \n",
    "        print(\"Recall = {}% \\nPrecision = {}% \\n F1-Score = {}% \\n\".format(recall*100, precision*100, F1))\n",
    "\n",
    "    return F1, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''Feed forward Neural Network with Stochastic Gradient descent '''\n",
    "\n",
    "class myNNet():    \n",
    "    ''' layers: A list denoting how many neurons each layer must contain. \n",
    "    The size of the list defines the number of layers.\n",
    "    Example [100,500,200,2] imples the input layer has 100 neurons, there are 2 hidden layers with 500 and 200 neurons each\n",
    "    and output layer has 2 neurons '''\n",
    "    \n",
    "    def __init__(self,layers, reg_lamda = 0.0001):        \n",
    "        #initialise the Weights and biases of the model\n",
    "        np.random.seed(1)\n",
    "        self.L = layers\n",
    "        self.num_layers = len(layers)\n",
    "        self.w = [] # weights\n",
    "        self.b = [] # bias\n",
    "        self.Lambda = reg_lamda # lambda hyper parameter for regularization\n",
    "        for i in range(len(layers) - 1):\n",
    "            self.w.append(2 * np.random.random((layers[i], layers[i+1])) - 1)\n",
    "            self.b.append(np.zeros((1,layers[i+1]), dtype=np.float64))\n",
    "    \n",
    "    def __str__(self):\n",
    "        \n",
    "        note = \"Neural Network Model containing:\"\n",
    "        for i in range(self.num_layers):\n",
    "            if (i != self.num_layers-1):    \n",
    "                note += (\"\\nlayer{} = {} Nodes {} Weights {} Biases\".format(i, self.L[i], self.w[i].shape, self.b[i].shape))\n",
    "            else:\n",
    "                note += (\"\\nlayer{} = {} Nodes\".format(i, self.L[i]))\n",
    "        return note\n",
    "    \n",
    "    \n",
    "    '''returns the softmax values of the layer '''\n",
    "    def __sftmax(self,nodes):\n",
    "        exps = np.exp(nodes - np.max(nodes))\n",
    "        return exps / np.sum(exps)\n",
    "\n",
    "    '''returns the nonLin transformation '''\n",
    "    def __Sigmoid(self,z):\n",
    "        z=np.array(z,dtype=np.float64)\n",
    "        return (np.tanh(z))        \n",
    "    \n",
    "    '''returns the derivative of the nonLin transformation function  '''\n",
    "    def __dSigmoid(self,z):\n",
    "        z=np.array(z,dtype=np.float64)\n",
    "        return (1-np.tanh(z)**2)\n",
    "        \n",
    "    \n",
    "    '''Feeds the given input vector into the Neural network and returns the predicted output\n",
    "    If getAllLayers is set True then all neurons and Z values are returned, this is needed while training. '''\n",
    "    def predict(self, dat_in, getAllLayers = False):\n",
    "        # input data is directly the neuron values of the 1st layer\n",
    "        neurons = [dat_in.reshape(1,self.L[0])]\n",
    "        z = []\n",
    "\n",
    "        for L in range(self.num_layers - 1):\n",
    "            z.append(neurons[L].dot(self.w[L]) + self.b[L])\n",
    "            if(L != (self.num_layers - 2)):\n",
    "                neurons.append( self.__Sigmoid(z[L]))\n",
    "            else: #for the last layer perform softmax instead of Sigmoid\n",
    "                neurons.append( self.__sftmax(z[L]))\n",
    "        \n",
    "        if(getAllLayers):\n",
    "            return neurons, z\n",
    "        else: \n",
    "            return neurons[-1]\n",
    "    \n",
    "    \n",
    "    ''' Calculates the cost value in the model. This is used for checking the progress in the model training '''\n",
    "    def calcCost(self, predicted, expected):\n",
    "        totalCost = 0\n",
    "        for p,e in zip(predicted, expected):\n",
    "            totalCost += (np.log(p)*e).sum()\n",
    "        #add regularization factor\n",
    "#         totalCost += (self.Lambda/2)*sum((w**2).sum() for w in self.w)\n",
    "        return totalCost/len(predicted)\n",
    "    \n",
    "    \n",
    "    '''The main method where the model is trained using back propagation. \n",
    "    learn_rate and reg_lamba are 2 hyper-parameters of the model\n",
    "    If debugPrintCycles is set to 0, nothing is printed. If it is set to any other value, \n",
    "     various training information is printed every debugPrintCycle for checking progress '''\n",
    "    def train(self, data, passes, learn_rate = 0.01, debugPrintCycles = 0):        \n",
    "        \n",
    "        print(\"\\nStarting training cycles...\") \n",
    "        data_size = len(data)\n",
    "        \n",
    "        # if debugPrints is requested then plot the F1 evaluation and Loss values at every debug cycle\n",
    "        if(debugPrintCycles):\n",
    "            loss_values = []\n",
    "            F1_scores = []\n",
    "            pass_idx = []\n",
    "            \n",
    "        for i in range(passes):\n",
    "            \n",
    "            #create mini batches of randomly shuffled training data for stochastic gradient descent\n",
    "            np.random.shuffle(data)\n",
    "            num_of_mini_batches = len(str(data_size))            \n",
    "            mini_batch_size = data_size//num_of_mini_batches           \n",
    "            mini_batches = [data[i:i+mini_batch_size] for i in range(num_of_mini_batches)]\n",
    "\n",
    "            outputs = []\n",
    "            # matrices to store nudges of backpropogation\n",
    "            b_nudges = [np.zeros(b.shape,dtype=np.float64) for b in self.b]\n",
    "            w_nudges = [np.zeros(w.shape,dtype=np.float64) for w in self.w]\n",
    "            \n",
    "            if(debugPrintCycles): j = 0\n",
    "            \n",
    "            for mini_batch in mini_batches:\n",
    "                for dat_in, dat_out in mini_batch:  \n",
    "\n",
    "                    # forward propagation    \n",
    "                    neurons,z = self.predict( dat_in, getAllLayers = True)                \n",
    "                    outputs.append(neurons[-1])\n",
    "\n",
    "                    if(debugPrintCycles):\n",
    "                        if not(i% debugPrintCycles): \n",
    "                            if(j == 0):\n",
    "                                print(\"\\nResults after {} training cycles : \".format(i))                                            \n",
    "                            j += 1\n",
    "\n",
    "                    # back propagation                           \n",
    "                    # get the partial derivatives for last layer\n",
    "                    dz = (outputs[-1] - dat_out)         \n",
    "                    b_nudges[-1] += dz\n",
    "                    w_nudges[-1] += neurons[-2].T.dot(dz)\n",
    "\n",
    "                    # get the partial derivatives for the rest of the layers\n",
    "                    for L in range(2, self.num_layers):\n",
    "                        dCost = dz.dot(self.w[-L+1].T) \n",
    "                        dz = self.__dSigmoid(z[-L]) * dCost\n",
    "                        b_nudges[-L] += dz\n",
    "                        w_nudges[-L] += neurons[-L-1].T.dot(dz) \n",
    "\n",
    "                # update the gradient descents learned\n",
    "                self.b = [(b-(learn_rate/mini_batch_size)*nb) for b, nb in zip(self.b, b_nudges)]\n",
    "                self.w = [(w-(self.Lambda/data_size)*w-(learn_rate/mini_batch_size)*nw) for w, nw in zip(self.w, w_nudges)]\n",
    "            \n",
    "            if(debugPrintCycles):\n",
    "                if not (i% debugPrintCycles):\n",
    "                    \n",
    "                    # calculate loss value\n",
    "                    expected = [dat_out for dat_in, dat_out in data]\n",
    "                    cost = self.calcCost(outputs, expected)\n",
    "                    print(\"Loss = {}\".format(cost))\n",
    "                    # calculate Accuracy \n",
    "                    F1,_ = evaluateModel(self, testVecs, -0.65)\n",
    "                    print(\"F1 = {}\".format(F1))\n",
    "\n",
    "                    pass_idx.append(i)\n",
    "                    loss_values.append(cost)\n",
    "                    F1_scores.append(F1)\n",
    "                    \n",
    "                    self.saveModel(str(i))\n",
    "        \n",
    "        print(\"Finished Training ! \")\n",
    "        self.saveModel(str(passes))\n",
    "        \n",
    "        if(debugPrintCycles): \n",
    "            # plot F1-scores and loss v/s training cycles \n",
    "            f, ax1 = plt.subplots()\n",
    "            ax1.set_title(\"NeuralNet Loss v/s training cycles\")\n",
    "            ax1.xlabel = \"Training cycles\"\n",
    "            ax1.ylabel = \"Loss in the model\"\n",
    "            ax1.plot(pass_idx, loss_values, 'b')\n",
    "            f, ax2 = plt.subplots()\n",
    "            ax2.set_title(\"F1-scores v/s training cycles\")\n",
    "            ax2.xlabel = \"Training cycles\"\n",
    "            ax2.ylabel = \"Accuracy of Prediction\"\n",
    "            ax2.plot(pass_idx, F1_scores, 'r')\n",
    "            plt.show()\n",
    "            \n",
    "    \n",
    "    '''Saves the model parameter in out files that can be loaded later using updateModel() '''\n",
    "    def saveModel(self, suffix):\n",
    "        saveFile = NN_MODEL_FILES + \"_\".join(str(l) for l in self.L[0:-1]) +\"_\" + suffix\n",
    "        \n",
    "        np.save(saveFile + \"_w\", self.w)\n",
    "        for b_i, b in enumerate(self.b):\n",
    "            np.save(saveFile + \"_b\"+ str(b_i), b)\n",
    "        print(\".. saved model params in {} files\".format(saveFile))\n",
    "        \n",
    "        \n",
    "    '''Method loads a previously trained model saved in npy files and updates the current parameters. '''\n",
    "    def updateModel(self, suffix):        \n",
    "        loadFile = \"../for_GPU/nnModel_\" + \"_\".join(str(l) for l in self.L[0:-1]) +\"_\" + suffix\n",
    "        \n",
    "        self.w = np.load(loadFile + \"_w\" + \".npy\")        \n",
    "        for b_i in range(len(self.b)):\n",
    "            self.b[b_i] = np.load(loadFile + \"_b\"+ str(b_i) + \".npy\")\n",
    "        print(\".. updated model with params from {} files\".format(loadFile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Model containing:\n",
      "layer0 = 1116 Nodes (1116, 100) Weights (1, 100) Biases\n",
      "layer1 = 100 Nodes (100, 10) Weights (1, 10) Biases\n",
      "layer2 = 10 Nodes (10, 2) Weights (1, 2) Biases\n",
      "layer3 = 2 Nodes\n",
      ".. updated model with params from ../for_GPU/nnModel_1116_100_10_800 files\n"
     ]
    }
   ],
   "source": [
    "# 2 hidden layers with 100 nodes and 10 nodes each\n",
    "layers = [nn_in_dem, 100, 10, nn_out_dem]\n",
    "\n",
    "segmentor = myNNet(layers)\n",
    "print(segmentor)\n",
    "\n",
    "# update parameters of the model from a previously trained model\n",
    "segmentor.updateModel(str(800))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "**Note: Donot Execute the below cell if not necessary **\n",
    "<br>The cell takes approx. **3 hours** for execution.<br>\n",
    "The results can still be viewed without generating the training vectors as a pre-trained model is provided.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model\n",
    "train_passes = 700\n",
    "# segmentor.train(trainingVecs, train_passes, debugPrintCycles = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP 1083,FN 371,FP 412,TN 18827\n",
      "Recall = 74.48418156808803% \n",
      "Precision = 72.44147157190636% \n",
      " F1-Score = 73.44862665310275% \n",
      "\n"
     ]
    }
   ],
   "source": [
    "F1_score, nn_predictions = evaluateModel(segmentor, testVecs, .01, printDebugs = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST CODE gen for Neural Network\n",
    "#generate simple data\n",
    "# np.random.seed(1)\n",
    "# data = []\n",
    "# for i in range(1,500): \n",
    "#     in_dat = 2*np.random.random((1,2)) -1 \n",
    "#     # A non-linear function to test on the Neural network \n",
    "#     if(((in_dat[0][0]**9 - np.exp(in_dat[0][1]))) > (in_dat[0][0])):\n",
    "#         out_dat = np.array([1,0]).reshape(1,2)\n",
    "#     else:\n",
    "#         out_dat = np.array([0,1]).reshape(1,2)\n",
    "        \n",
    "#     data.append((in_dat,out_dat))\n",
    "\n",
    "# training = data[:-20]\n",
    "# test = data[-20:]\n",
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### TEST CODE run for Neural Network\n",
    "# testLayers = [2, 4, 2]\n",
    "# testModel = myNNet(testLayers)\n",
    "# print(testModel)\n",
    "# testModel.train(training, passes = 2000, debugPrintCycles = 0 )\n",
    "# # test the model's accuracy.\n",
    "# for in_dat, out_dat in test:\n",
    "#     print(\"\\nExpected test out : {}\\n Predicted test out : {}\\n\".format(out_dat, testModel.predict(in_dat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EDU-seperated files successfully generated in data/results folder.\n"
     ]
    }
   ],
   "source": [
    "'''Function generates corpus files incorporating the EDU breaks predicted by the model.\n",
    "The EDU break is represented by a newline in the file.\n",
    "The files are generated with the same file name but with an extension .edus '''\n",
    "def gen_Segmentation_Results(fnames, tokens, predictions):\n",
    "    \n",
    "    filesDict = defaultdict(str)\n",
    "    sent_count = 0\n",
    "    word_count = 0\n",
    "    \n",
    "    for sent_i, sent in enumerate(tokens):\n",
    "        edu_sent = \"\"\n",
    "        for word_i, word in enumerate(sent):\n",
    "            edu_sent += \" \" + word\n",
    "            \n",
    "            # if it's the last word in the sent then write to dict and move on\n",
    "            if(word_i == (len(sent)-1)):                \n",
    "                file = fnames[word_count]\n",
    "                filesDict[file] += (edu_sent + \"\\n\")\n",
    "                \n",
    "            #if an EDU break is predicted by the model then add a newline in the file\n",
    "            elif(predictions[word_i + word_count - sent_count] == 'B'):\n",
    "                edu_sent += \"\\n\"\n",
    "                \n",
    "        word_count += len(sent)\n",
    "        sent_count += 1\n",
    "        \n",
    "    allFiles = sorted(set(fnames))\n",
    "    for fname in allFiles:      \n",
    "        with open(PATH + \"results/\"+fname+\".edus\", \"w+\") as f:\n",
    "                  f.write(filesDict[fname])\n",
    "    print(\"EDU-seperated files successfully generated in data/results folder.\")\n",
    "\n",
    "#########################################################################################################################\n",
    "#get the file_names in the sorted order\n",
    "df = pd.read_csv(\"data_set_1__test.csv\", usecols=[\"file_name\",\"word\"])\n",
    "test_fileNames = list(df.file_name)\n",
    "\n",
    "gen_Segmentation_Results(test_fileNames, tokenized_test_data, nn_predictions )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
