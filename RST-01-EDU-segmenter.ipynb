{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading corpus from /media/roshanrane/DATA/Study/Masters/Workspace/ANLP/ANLP_assignmentFinal/repo/RST-parser/../../RST_corpus/data/RSTtrees-WSJ-main-1.0/TRAINING/\n",
      "Reading corpus from /media/roshanrane/DATA/Study/Masters/Workspace/ANLP/ANLP_assignmentFinal/repo/RST-parser/../../RST_corpus/data/RSTtrees-WSJ-main-1.0/TEST/\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "import glob\n",
    "import nltk\n",
    "\n",
    "'''\n",
    "Reads the RST-DT corpus data.\n",
    "The function returns a list of sentences tokenized to words and index of an EDU break in each of these sentence\n",
    "\n",
    "Attention : The internal folder structure of the corpus must not be altered. \n",
    "'''\n",
    "def readCorpus(relDir, dataClass):    \n",
    "    #go to the corpus dir  \n",
    "    if((dataClass.upper() != \"TRAINING\") and (dataClass.upper() != \"TEST\")):\n",
    "        raise ValueError(\"Value of 'dataClass' is incorrect. Select one of the following - 1)'TRAINING' 2)'TEST'\")\n",
    "    else:\n",
    "        relDir +=  \"/data/RSTtrees-WSJ-main-1.0/\" + dataClass.upper() + \"/\"\n",
    "        fileext = \"*.edus\"        \n",
    " \n",
    "    absDir = os.path.join(os.getcwd(), relDir)\n",
    "    \n",
    "    if(os.path.isdir(absDir)):\n",
    "        print(\"Reading corpus from \"+ absDir)\n",
    "    else:\n",
    "        raise ValueError(\"The dir \" + absDir +\" is incorrect or doesnot exist. Please check check the value set in 'relDir' \")\n",
    "            \n",
    "    files = os.path.join(absDir, fileext)\n",
    "    \n",
    "    tokens = []\n",
    "    edu_idx = []\n",
    "    for fname in glob.glob(files):\n",
    "        with open(fname, 'r', encoding='utf-8') as doc:\n",
    "            sent=[]\n",
    "            edu_boundary = []\n",
    "            for edu in doc:\n",
    "                # tokenize to words\n",
    "                edu_tokens = nltk.word_tokenize(edu)\n",
    "                \n",
    "                if edu_tokens[-1] not in [\"!\", \"?\", \".\", \"...\"]:\n",
    "                    # join EDUs of a sentence\n",
    "                    sent.extend(edu_tokens)\n",
    "                    # remember EDU boundary indices\n",
    "                    edu_boundary.append(len(sent) - 1)\n",
    "                else:\n",
    "                    sent.extend(edu_tokens)\n",
    "                    tokens.append(sent)\n",
    "                    edu_idx.append(edu_boundary)\n",
    "                    # clear for next sentence\n",
    "                    sent = []\n",
    "                    edu_boundary = []\n",
    "            \n",
    "    return tokens,edu_idx\n",
    "\n",
    "############################################################################################\n",
    "\n",
    "tokenized_train_data,train_EDUs = readCorpus(\"../../RST_corpus\", \"Training\")\n",
    "tokenized_test_data,test_EDUs = readCorpus(\"../../RST_corpus\", \"Test\")\n",
    "\n",
    "### TEST CODE\n",
    "# for i in range(len(tokenized_test_data)):\n",
    "#     print(\"-------------------\")\n",
    "#     print(tokenized_test_data[i])\n",
    "#     print(\"word count = {}\".format(len(tokenized_test_data[i])))\n",
    "#     print(test_EDUs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Word2Vec and Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of training vecs = 176315 with 4-gram grouping \n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#constants denoting the n-gram and output vectors used in the whole project \n",
    "N_GRAM = 4\n",
    "B_ARRAY = np.array([1,0])\n",
    "C_ARRAY = np.array([0,1])\n",
    "\n",
    "'''Plot the Word2Vec in 2D for visualization'''\n",
    "def w2v_visualizer(model):\n",
    "    labels = []\n",
    "    tokens = []\n",
    "    for word in model.wv.vocab:\n",
    "        tokens.append(model[word])\n",
    "        labels.append(word)\n",
    "        \n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "    x = [value[0] for value in new_values]\n",
    "    y = [value[1] for value in new_values] \n",
    "    \n",
    "    plt.figure(figsize=(20, 16)) \n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i], xy=(x[i], y[i]), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom')\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "'''Generate Word2Vec for all words in the corpus'''\n",
    "def gen_w2v(tokenized_data, visualize = False):\n",
    "    # Word2Vec - Convert each word token to a vector of size 100\n",
    "    model = gensim.models.Word2Vec(tokenized_data, size= 100 , window=20, min_count=1)\n",
    "    model.save(\"word2vec-RSTCorpus\")\n",
    "    if(visualize):\n",
    "        w2v_visualizer(model)   \n",
    "    return model\n",
    "    \n",
    "    \n",
    "''' Takes Corpus extractions and EDU boundaries and converts it to input vectors and \n",
    "output vectors to feed to the Neural Network.\n",
    "Concatenate the words in n-grams for input and calculates the Word Embeddings for each word.\n",
    "Output is a 2-element vector : [1,-1] meaning EDU Break\n",
    "                             : [-1,1] meaning no EDU Break '''\n",
    "def extract_nnVec(tokenized_data, edu_lists, model, n_gram):\n",
    "    nnData = []    \n",
    "    for sent,EDUs in zip(tokenized_data, edu_lists) :\n",
    "        \n",
    "        #Extend the sentences with '...' This will ensure that one n-gram sequence \n",
    "        # exists for every word pair in the original sentence, with the word pair as the mid point in it.\n",
    "        ext = [\".\" for i in range((n_gram//2) -1)]\n",
    "        extended_sent = ext + sent + ext\n",
    "#             print(\" \".join(extended_sent))\n",
    "            \n",
    "            \n",
    "        for i in range(len(extended_sent)- n_gram + 1):            \n",
    "            #get wordvec of each word of n-gram and stack them\n",
    "            nnDat_in = model.wv[extended_sent[i]]\n",
    "            for j in range(1,n_gram):                \n",
    "                nnDat_in = np.hstack((nnDat_in, model.wv[extended_sent[i+j]] + df.loc(i+j))\n",
    "            \n",
    "            #check if there is a EDU break in between the n-gram.\n",
    "            mid_index = (i + n_gram//2 - 1)\n",
    "            \n",
    "            if(mid_index in EDUs):\n",
    "#                 print(\"{} {} {} {}\".format(sent[i],sent[i+1], sent[i+2], sent[i+3]))\n",
    "                nnDat_out = B_ARRAY # EDU 'Break' class\n",
    "            else:\n",
    "                nnDat_out = C_ARRAY # EDU 'Continue' class\n",
    "                \n",
    "            nnData.append((nnDat_in, nnDat_out))\n",
    "            \n",
    "    return nnData\n",
    "    \n",
    "##########################################################################################\n",
    "\n",
    "# word vectors are generated for the whole corpus \n",
    "w2v_model = gen_w2v(tokenized_train_data + tokenized_test_data)\n",
    "#generate Vectors for training and test data\n",
    "trainingVecs = extract_nnVec(tokenized_train_data, train_EDUs, w2v_model, n_gram = N_GRAM)\n",
    "testVecs = extract_nnVec(tokenized_test_data, test_EDUs, w2v_model, n_gram = N_GRAM)\n",
    "\n",
    "nn_in_dem = trainingVecs[0][0].shape[0]\n",
    "nn_out_dem = 2\n",
    "\n",
    "del w2v_model\n",
    "# print(\"number of words = {}\".format(len([word for sent in tokenized_train_data for word in sent])))\n",
    "# print(\"number of sentences = \",len(tokenized_train_data))\n",
    "print(\"number of training vecs = {} with {}-gram grouping \".format(len(trainingVecs), N_GRAM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "'''Feed forward Neural Network with Stochastic Gradient descent '''\n",
    "\n",
    "class myNNet():    \n",
    "    ''' layers: A list containing denoting how many neurons each layer must contain. \n",
    "    The size of the list defines the number of layers.\n",
    "    Example [100,500,200,2] imples the input layer has 100 neurons, there are 2 hidden layers with '''\n",
    "    \n",
    "    def __init__(self,layers):        \n",
    "        #initialise the Weights and biases of the model\n",
    "        np.random.seed(1)\n",
    "        self.L = layers\n",
    "        self.num_layers = len(layers)\n",
    "        self.w = [] # weights\n",
    "        self.b = [] # bias\n",
    "        for i in range(len(layers) - 1):\n",
    "            self.w.append(2 * np.random.random((layers[i], layers[i+1])) - 1)\n",
    "            self.b.append(np.zeros((1,layers[i+1])))\n",
    "    \n",
    "    def __str__(self):\n",
    "        \n",
    "        note = \"Neural Network Model containing:\"\n",
    "        for i in range(self.num_layers):\n",
    "            if (i != self.num_layers-1):    \n",
    "                note += (\"\\nlayer{} = {} Nodes {} Weights {} Biases\".format(i, self.L[i], self.w[i].shape, self.b[i].shape))\n",
    "            else:\n",
    "                note += (\"\\nlayer{} = {} Nodes\".format(i, self.L[i]))\n",
    "        return note\n",
    "    \n",
    "    \n",
    "    '''returns the softmax values of the layer '''\n",
    "    def __sftmax(self,nodes):\n",
    "        exps = np.exp(nodes - np.max(nodes))\n",
    "        return exps / np.sum(exps)\n",
    "    \n",
    "    '''returns the derivative of the sigmoid function - in my case the tanh function '''\n",
    "    def __dSigma(self,z):\n",
    "        return (1 - (np.tanh(z)**2))\n",
    "        \n",
    "    \n",
    "    '''Feeds the given input vector into the Neural network and returns the predicted output\n",
    "    If getAllLayers is set True then all neurons and Z values are returned, this is needed while training. '''\n",
    "    def predict(self, dat_in, getAllLayers = False):\n",
    "        # input data is directly the neuron values of the 1st layer\n",
    "        neurons = [dat_in.reshape(1,self.L[0])]\n",
    "        z = []\n",
    "\n",
    "        for L in range(self.num_layers - 1):\n",
    "            z.append(neurons[L].dot(self.w[L]) + self.b[L])\n",
    "            if(L != (self.num_layers - 2)):\n",
    "                neurons.append( np.tanh(z[L]))\n",
    "            else: #for the last layer perform softmax instead of tanH \n",
    "                neurons.append( self.__sftmax(z[L]))\n",
    "        \n",
    "        if(getAllLayers):\n",
    "            return neurons, z\n",
    "        else: \n",
    "            return neurons[-1]\n",
    "    \n",
    "    \n",
    "    ''' Calculates the cost value in the model. This is used for checking the progress in the model training '''\n",
    "    def calcCost(self, predicted, expected):\n",
    "        totalCost = 0\n",
    "        for p,e in zip(predicted, expected):\n",
    "            totalCost += -(np.log(p)*e).sum()\n",
    "        return totalCost/len(predicted)\n",
    "    \n",
    "    \n",
    "    '''The main method where the model is trained using back propagation. \n",
    "    If debugPrintCycles is set to 0, nothing is printed. If it is set to any other value, \n",
    "    the various parameters are printed every debugPrintCycle times '''\n",
    "    def train(self, data, passes, learn_rate = 0.01, debugPrintCycles = 0):        \n",
    "        \n",
    "        print(\"\\nStarting training cycles...\") \n",
    "        data_size = len(data)\n",
    "        for i in range(passes):\n",
    "            \n",
    "            #create mini batches of randomly shuffled training data for stochastic gradient descent\n",
    "            np.random.shuffle(data)\n",
    "            num_of_mini_batches = len(str(data_size))            \n",
    "            mini_batch_size = data_size//num_of_mini_batches           \n",
    "            mini_batches = [data[i:i+mini_batch_size] for i in range(num_of_mini_batches)]\n",
    "\n",
    "            outputs = []\n",
    "            # matrices to store nudges of backpropogation\n",
    "            b_nudges = [np.zeros(b.shape) for b in self.b]\n",
    "            w_nudges = [np.zeros(w.shape) for w in self.w]\n",
    "            \n",
    "            if(debugPrintCycles): j = 0\n",
    "            \n",
    "            for mini_batch in mini_batches:\n",
    "                for dat_in, dat_out in mini_batch:  \n",
    "\n",
    "                    # forward propagation    \n",
    "                    neurons,z = self.predict( dat_in, getAllLayers = True)                \n",
    "                    outputs.append(neurons[-1])\n",
    "\n",
    "                    if(debugPrintCycles):\n",
    "                        if not(i% debugPrintCycles): \n",
    "                            if(j == 0):\n",
    "                                print(\"\\nResults after {} training cycles : \".format(i))\n",
    "                            elif(j < 10): # print out the first 10 predictions\n",
    "                                print(\"Expected out = \",dat_out)\n",
    "                                print(\"Predicted out = \",outputs[-1])                                                   \n",
    "                            j += 1\n",
    "\n",
    "                    # back propagation                           \n",
    "                    # get the partial derivatives for last layer\n",
    "                    dz = (outputs[-1] - dat_out)         \n",
    "                    b_nudges[-1] += dz\n",
    "                    w_nudges[-1] += neurons[-2].T.dot(dz)\n",
    "\n",
    "                    # get the partial derivatives for the rest of the layers\n",
    "                    for L in range(2, self.num_layers):\n",
    "                        dCost = dz.dot(self.w[-L+1].T) \n",
    "                        dz = self.__dSigma(z[-L]) * dCost\n",
    "                        b_nudges[-L] += dz\n",
    "                        w_nudges[-L] += neurons[-L-1].T.dot(dz)\n",
    "\n",
    "                # update the gradient descents learned\n",
    "                self.b = [(b-(learn_rate/mini_batch_size)*nb) for b, nb in zip(self.b, b_nudges)]\n",
    "                self.w = [(w-(learn_rate/mini_batch_size)*nw) for w, nw in zip(self.w, w_nudges)]\n",
    "            \n",
    "            if(debugPrintCycles):\n",
    "                if not (i% debugPrintCycles):\n",
    "                    # print loss\n",
    "                    expected = [dat_out for dat_in, dat_out in data]\n",
    "                    print(\"Loss = {}\".format(self.calcCost(outputs, expected)))\n",
    "        \n",
    "        print(\"Finished Training ! \")\n",
    "            \n",
    "    \n",
    "    '''Saves the model parameter in out files that can be loaded later using updateModel() '''\n",
    "    def saveModel(self, suffix):\n",
    "        saveFile = 'nnModel_'+\"_\".join(str(l) for l in self.L[0:-1]) +\"_\" + suffix\n",
    "        \n",
    "        np.save(saveFile + \"_w\", self.w)\n",
    "        for b_i, b in enumerate(self.b):\n",
    "            np.save(saveFile + \"_b\"+ str(b_i), b)\n",
    "        print(\".. saved model params in {} files\".format(saveFile))\n",
    "        \n",
    "        \n",
    "    '''Method loads a previously trained model saved in npy files and updates the current parameters. '''\n",
    "    def updateModel(self, suffix):        \n",
    "        loadFile = 'nnModel_'+\"_\".join(str(l) for l in self.L[0:-1]) +\"_\" + suffix\n",
    "        \n",
    "        self.w = np.load(loadFile + \"_w\" + \".npy\")        \n",
    "        for b_i in range(len(self.b)):\n",
    "            self.b[b_i] = np.load(loadFile + \"_b\"+ str(b_i) + \".npy\")\n",
    "        print(\".. updated model with params from {} files\".format(loadFile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Model containing:\n",
      "layer0 = 400 Nodes (400, 50) Weights (1, 50) Biases\n",
      "layer1 = 50 Nodes (50, 10) Weights (1, 10) Biases\n",
      "layer2 = 10 Nodes (10, 2) Weights (1, 2) Biases\n",
      "layer3 = 2 Nodes\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 139 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 2 hidden layers with 50 nodes and 10 nodes each\n",
    "# layers = [nn_in_dem, 50, 10, nn_out_dem]\n",
    "\n",
    "# segmentor = myNNet(layers)\n",
    "print(segmentor)\n",
    "\n",
    "#train model\n",
    "train_passes = 1000\n",
    "# update parameters of the model from previously trained model, if any\n",
    "# segmentor.updateModel(str(11))\n",
    "\n",
    "# segmentor.train(trainingVecs, train_passes, debugPrintCycles = 333)\n",
    "# #save model to save re-training time\n",
    "# segmentor.saveModel(str(train_passes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST CODE gen for Neural Network\n",
    "#generate simple data\n",
    "# np.random.seed(1)\n",
    "# data = []\n",
    "# for i in range(1,500): \n",
    "#     in_dat = 2*np.random.random((1,2)) -1 \n",
    "#     # A non-linear function to test on the Neural network \n",
    "#     if(((in_dat[0][0]**9 - np.exp(in_dat[0][1]))) > (in_dat[0][0])):\n",
    "#         out_dat = np.array([1,0]).reshape(1,2)\n",
    "#     else:\n",
    "#         out_dat = np.array([0,1]).reshape(1,2)\n",
    "        \n",
    "#     data.append((in_dat,out_dat))\n",
    "\n",
    "# training = data[:-20]\n",
    "# test = data[-20:]\n",
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### TEST CODE run for Neural Network\n",
    "# testLayers = [2, 4, 2]\n",
    "# testModel = myNNet(testLayers)\n",
    "# print(testModel)\n",
    "# testModel.train(training, passes = 2000, debugPrintCycles = 0 )\n",
    "# # test the model's accuracy.\n",
    "# for in_dat, out_dat in test:\n",
    "#     print(\"\\nExpected test out : {}\\n Predicted test out : {}\\n\".format(out_dat, testModel.predict(in_dat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data size = 20693\n",
      "Total EDU breaks expected = 1454\n",
      "EDU breaks count again =  1449\n",
      "TP 497,FN 952,FP 679,TN 18565\n",
      "Recall = 34.29951690821256% \n",
      "Precision = 42.26190476190476% \n",
      " F1-Score = 37.866666666666674% \n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''Using the output vector of the Neural Network, decides if the prediction is a EDU 'Break' or not '''\n",
    "def extractResult(nn_out):\n",
    "    threshold  = -0.55\n",
    "    if((nn_out[0] - nn_out[1]) > threshold ):\n",
    "        return \"B\"\n",
    "    else:\n",
    "        return \"C\"\n",
    "\n",
    "    \n",
    "# Predict the EDU breaks on the test data\n",
    "print(\"Test data size = {}\".format(len(testVecs)))\n",
    "print(\"Total EDU breaks expected = {}\".format(sum([len(edu) for edu in test_EDUs])))\n",
    "\n",
    "# count the True Positives, False Positives, True Negatives and False Negatives\n",
    "TP = FP = TN = FN = 0\n",
    "c=0\n",
    "for in_vec, expected_vec in testVecs:\n",
    "    # pass the test data through the neural network model\n",
    "    predicted_vec = segmentor.predict(in_vec)    \n",
    "    predicted = extractResult(predicted_vec.reshape(-1))  \n",
    "    if(np.array_equal(expected_vec, B_ARRAY)):\n",
    "        expected = 'B'\n",
    "        c += 1\n",
    "    else:\n",
    "        expected = 'C'        \n",
    "       \n",
    "#     print(\"predicted = \" , predicted_vec)\n",
    "#     print(\"expected = \" , expected_vec)  \n",
    "#     print(\"----------\")\n",
    "    \n",
    "    if((expected == 'B') and (predicted == 'B')):\n",
    "        TP += 1\n",
    "    elif((expected == 'B') and (predicted == 'C')):\n",
    "        FN += 1\n",
    "    elif((expected == 'C') and (predicted == 'B')):\n",
    "        FP += 1\n",
    "    elif((expected == 'C') and (predicted == 'C')):\n",
    "        TN += 1\n",
    "        \n",
    "print(\"EDU breaks count again = \",c)\n",
    "\n",
    "#recall, Precision and F1-score calculation\n",
    "print(\"TP {},FN {},FP {},TN {}\".format(TP,FN,FP,TN))\n",
    "\n",
    "recall = TP/(TP+FN)\n",
    "precision = TP/(TP+FP)\n",
    "F1 = 2*recall*precision/(recall + precision)      \n",
    "\n",
    "print(\"Recall = {}% \\nPrecision = {}% \\n F1-Score = {}% \\n\".format(recall*100, precision*100, F1*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "outList[0].sum?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ") learn_rate = 0.01 results in local minima too quickly. set to 0.01/num_of_training_data\n",
    ") added softmax() \n",
    ") switch from square mean loss function to cross entropy loss function.\n",
    ") Stochastic descent - \n",
    "without stochastic descent:\n",
    "10 training cycles complete...\n",
    "Cost = 0.28354821176544787\n",
    "Wall time: 2min 33s\n",
    "\n",
    "with stochastic descent:\n",
    "10 training cycles complete...\n",
    "Cost = 0.26668334795269005\n",
    "Wall time: 2min 25s\n",
    "\n",
    "\n",
    "Neural Network Model containing:\n",
    "layer0 = 400 Nodes (400, 50) Weights (1, 50) Biases\n",
    "layer1 = 50 Nodes (50, 10) Weights (1, 10) Biases\n",
    "layer2 = 10 Nodes (10, 2) Weights (1, 2) Biases\n",
    "layer3 = 2 Nodes\n",
    "\n",
    "1500 training cycles at a learning rate 0.000005 :\n",
    "F1-score 37%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
