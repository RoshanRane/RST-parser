{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-0.22.0-cp36-cp36m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (14.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 14.9MB 77kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy>=1.9.0 (from pandas)\n",
      "  Downloading numpy-1.14.1-cp36-cp36m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl (4.7MB)\n",
      "\u001b[K    100% |████████████████████████████████| 4.7MB 244kB/s ta 0:00:011\n",
      "\u001b[?25hCollecting pytz>=2011k (from pandas)\n",
      "  Downloading pytz-2018.3-py2.py3-none-any.whl (509kB)\n",
      "\u001b[K    100% |████████████████████████████████| 512kB 1.7MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already up-to-date: python-dateutil>=2 in /Users/YK/anaconda3/lib/python3.6/site-packages (from pandas)\n",
      "Requirement already up-to-date: six>=1.5 in /Users/YK/anaconda3/lib/python3.6/site-packages (from python-dateutil>=2->pandas)\n",
      "Installing collected packages: numpy, pytz, pandas\n",
      "  Found existing installation: numpy 1.13.3\n",
      "    Uninstalling numpy-1.13.3:\n",
      "      Successfully uninstalled numpy-1.13.3\n",
      "  Found existing installation: pytz 2017.2\n",
      "    Uninstalling pytz-2017.2:\n",
      "      Successfully uninstalled pytz-2017.2\n",
      "  Found existing installation: pandas 0.20.3\n",
      "    Uninstalling pandas-0.20.3:\n",
      "      Successfully uninstalled pandas-0.20.3\n",
      "Successfully installed numpy-1.14.1 pandas-0.22.0 pytz-2018.3\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Downloading tqdm-4.19.5-py2.py3-none-any.whl (51kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 1.9MB/s ta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: tqdm\n",
      "Successfully installed tqdm-4.19.5\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import gensim\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# constructing the tree: string to sequence of its elements\n",
    "def split(tree_str):\n",
    "    chunks = tree_str.split()\n",
    "    chunk_elements = []\n",
    "    for chunk in chunks:\n",
    "        if chunk[0] == \"(\":\n",
    "            index_i = 0\n",
    "            while chunk[index_i] == \"(\":\n",
    "                chunk_elements.append(\"(\")\n",
    "                index_i += 1\n",
    "            chunk_elements.append(chunk[index_i:])\n",
    "        else:\n",
    "            index_j = (-1)\n",
    "            while chunk[index_j] == \")\":\n",
    "                chunk_elements.append(\")\")\n",
    "                index_j += (-1)\n",
    "            chunk_elements.insert((index_j + 1), chunk[0:((index_j)+1)])\n",
    "    return(chunk_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generating constituents\n",
    "def constituents(tree_str):\n",
    "# nt = non-terminal\n",
    "    nt_seq = []\n",
    "    term_counter = 0\n",
    "    constituents_set = set()\n",
    "    parsed_str = split(tree_str)\n",
    "    element_index = 0\n",
    "    none_depth = 0\n",
    "    depth = 0\n",
    "    while element_index < len(parsed_str):\n",
    "        if parsed_str[element_index] == \"(\":\n",
    "            nt = parsed_str[element_index + 1]\n",
    "            depth += 1\n",
    "            if nt == \"-NONE-\":\n",
    "# we are entering a subtree beginning with -NONE-                \n",
    "                none_depth += 1\n",
    "                nt_seq.append((nt, term_counter + 1))\n",
    "            else:\n",
    "# taking first of the nt-label\n",
    "                if len(nt) > 0 and nt[0] == \"-\":\n",
    "                    first_nt_label = nt\n",
    "                else:\n",
    "                    first_nt_label = nt.split(\"-\")[0]\n",
    "                nt_seq.append((first_nt_label, term_counter + 1))\n",
    "#  skipping a non-terminal           \n",
    "            element_index += 2\n",
    "        elif parsed_str[element_index] == \")\":\n",
    "            last_nt = nt_seq.pop()\n",
    "            depth -= 1\n",
    "            if last_nt[0] == \"-NONE-\":\n",
    "# exiting a subtree beginning with -NONE-                \n",
    "                none_depth -= 1\n",
    "            else:\n",
    "                if last_nt[1] <= term_counter:\n",
    "                    if last_nt[0] == \"ROOT\" or last_nt[0] ==\"TOP\":\n",
    "                        pass\n",
    "                    else:\n",
    "                        constituents_set.add((last_nt[0], last_nt[1], term_counter, depth))\n",
    "                else:\n",
    "# In this case all terminals in the subtree were ignored, which means that the subtree contained nothing but traces, \n",
    "# therefore we're skipping the constituent.\n",
    "                    pass\n",
    "            element_index += 1\n",
    "        else:\n",
    "            if none_depth == 0:\n",
    "# if we are not in a subtree beginning with -NONE-, then we count the terminal  \n",
    "                term_counter += 1\n",
    "#                 print(term_counter, parsed_str[element_index])\n",
    "            else:\n",
    "# otherwise we ignore the terminal            \n",
    "                pass\n",
    "            element_index += 1\n",
    "    return constituents_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"edu_segmentation/\" + \"file1\" + \".penn\") as file:\n",
    "    trees_string = file.read()\n",
    "    trees = trees_string.split(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_word_constituents = []\n",
    "for tree in trees:\n",
    "    tree_dict = {}\n",
    "    for constituent in constituents(tree):\n",
    "        if constituent[1] not in tree_dict:\n",
    "            tree_dict[constituent[1]] = ([],[])\n",
    "        tree_dict[constituent[1]][0].append(constituent)\n",
    "        if constituent[2] not in tree_dict:\n",
    "            tree_dict[constituent[2]] = ([],[])\n",
    "        second_constituent = tree_dict[constituent[2]][1].append(constituent)\n",
    "    one_word_constituents.append(tree_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: ([('S', 1, 78, 1), ('NP', 1, 8, 2), ('NP', 1, 2, 3), ('JJ', 1, 1, 4)],\n",
       "  [('JJ', 1, 1, 4)]),\n",
       " 2: ([('NN', 2, 2, 4)], [('NN', 2, 2, 4), ('NP', 1, 2, 3)]),\n",
       " 3: ([('CC', 3, 3, 3)], [('CC', 3, 3, 3)]),\n",
       " 4: ([('NP', 4, 8, 3), ('NNP', 4, 4, 4)], [('NNP', 4, 4, 4)]),\n",
       " 5: ([('JJ', 5, 5, 4)], [('JJ', 5, 5, 4)]),\n",
       " 6: ([('NN', 6, 6, 4)], [('NN', 6, 6, 4)]),\n",
       " 7: ([('NN', 7, 7, 4)], [('NN', 7, 7, 4)]),\n",
       " 8: ([('NNP', 8, 8, 4)], [('NNP', 8, 8, 4), ('NP', 4, 8, 3), ('NP', 1, 8, 2)]),\n",
       " 9: ([('VP', 9, 77, 2), ('VBZ', 9, 9, 3)], [('VBZ', 9, 9, 3)]),\n",
       " 10: ([('ADVP', 10, 10, 3), ('RB', 10, 10, 4)],\n",
       "  [('ADVP', 10, 10, 3), ('RB', 10, 10, 4)]),\n",
       " 11: ([('VBN', 11, 11, 4), ('VP', 11, 77, 3)], [('VBN', 11, 11, 4)]),\n",
       " 12: ([('NP', 12, 24, 4), ('NNP', 12, 12, 6), ('NP', 12, 12, 5)],\n",
       "  [('NNP', 12, 12, 6), ('NP', 12, 12, 5)]),\n",
       " 13: ([('VBN', 13, 13, 6), ('VP', 13, 24, 5)], [('VBN', 13, 13, 6)]),\n",
       " 14: ([('PP', 14, 18, 6), ('RB', 14, 14, 8), ('ADVP', 14, 14, 7)],\n",
       "  [('RB', 14, 14, 8), ('ADVP', 14, 14, 7)]),\n",
       " 15: ([('IN', 15, 15, 7)], [('IN', 15, 15, 7)]),\n",
       " 16: ([('IN', 16, 16, 7)], [('IN', 16, 16, 7)]),\n",
       " 17: ([('NP', 17, 18, 7), ('NNP', 17, 17, 8)], [('NNP', 17, 17, 8)]),\n",
       " 18: ([('NNP', 18, 18, 8)],\n",
       "  [('NP', 17, 18, 7), ('PP', 14, 18, 6), ('NNP', 18, 18, 8)]),\n",
       " 19: ([('PP', 19, 24, 6), ('IN', 19, 19, 7)], [('IN', 19, 19, 7)]),\n",
       " 20: ([('IN', 20, 20, 7)], [('IN', 20, 20, 7)]),\n",
       " 21: ([('NP', 21, 24, 7), ('DT', 21, 21, 8)], [('DT', 21, 21, 8)]),\n",
       " 22: ([('NNP', 22, 22, 8)], [('NNP', 22, 22, 8)]),\n",
       " 23: ([('JJ', 23, 23, 8)], [('JJ', 23, 23, 8)]),\n",
       " 24: ([('NN', 24, 24, 8)],\n",
       "  [('NP', 12, 24, 4),\n",
       "   ('VP', 13, 24, 5),\n",
       "   ('NP', 21, 24, 7),\n",
       "   ('PP', 19, 24, 6),\n",
       "   ('NN', 24, 24, 8)]),\n",
       " 25: ([('CD', 25, 25, 6), ('PP', 25, 43, 4), ('NP', 25, 27, 5)],\n",
       "  [('CD', 25, 25, 6)]),\n",
       " 26: ([('NN', 26, 26, 6)], [('NN', 26, 26, 6)]),\n",
       " 27: ([('NNS', 27, 27, 6)], [('NNS', 27, 27, 6), ('NP', 25, 27, 5)]),\n",
       " 28: ([('ADVP', 28, 28, 6), ('RB', 28, 28, 7), ('PP', 28, 34, 5)],\n",
       "  [('ADVP', 28, 28, 6), ('RB', 28, 28, 7)]),\n",
       " 29: ([('IN', 29, 29, 6)], [('IN', 29, 29, 6)]),\n",
       " 30: ([('IN', 30, 30, 6)], [('IN', 30, 30, 6)]),\n",
       " 31: ([('DT', 31, 31, 7), ('NP', 31, 34, 6)], [('DT', 31, 31, 7)]),\n",
       " 32: ([('NNP', 32, 32, 7)], [('NNP', 32, 32, 7)]),\n",
       " 33: ([('NNP', 33, 33, 7)], [('NNP', 33, 33, 7)]),\n",
       " 34: ([('NNP', 34, 34, 7)],\n",
       "  [('NNP', 34, 34, 7), ('PP', 28, 34, 5), ('NP', 31, 34, 6)]),\n",
       " 35: ([('PRN', 35, 43, 5), ('IN', 35, 35, 7), ('PP', 35, 42, 6)],\n",
       "  [('IN', 35, 35, 7)]),\n",
       " 36: ([('IN', 36, 36, 8), ('PP', 36, 42, 7)], [('IN', 36, 36, 8)]),\n",
       " 37: ([('DT', 37, 37, 9), ('NP', 37, 42, 8)], [('DT', 37, 37, 9)]),\n",
       " 38: ([('NN', 38, 38, 9)], [('NN', 38, 38, 9)]),\n",
       " 39: ([('NN', 39, 39, 9)], [('NN', 39, 39, 9)]),\n",
       " 40: ([('NNP', 40, 40, 9)], [('NNP', 40, 40, 9)]),\n",
       " 41: ([('NN', 41, 41, 9)], [('NN', 41, 41, 9)]),\n",
       " 42: ([('NNP', 42, 42, 9)],\n",
       "  [('NP', 37, 42, 8),\n",
       "   ('NNP', 42, 42, 9),\n",
       "   ('PP', 35, 42, 6),\n",
       "   ('PP', 36, 42, 7)]),\n",
       " 43: ([(',', 43, 43, 6)],\n",
       "  [('PRN', 35, 43, 5), ('PP', 25, 43, 4), (',', 43, 43, 6)]),\n",
       " 44: ([(',', 44, 44, 4)], [(',', 44, 44, 4)]),\n",
       " 45: ([('FRAG', 45, 77, 4), ('CC', 45, 45, 5)], [('CC', 45, 45, 5)]),\n",
       " 46: ([('NP', 46, 46, 5), ('NNP', 46, 46, 6)],\n",
       "  [('NP', 46, 46, 5), ('NNP', 46, 46, 6)]),\n",
       " 47: ([('S', 47, 67, 7),\n",
       "   ('NP', 47, 47, 8),\n",
       "   ('SBAR', 47, 76, 5),\n",
       "   ('PRP', 47, 47, 9),\n",
       "   ('S', 47, 76, 6)],\n",
       "  [('NP', 47, 47, 8), ('PRP', 47, 47, 9)]),\n",
       " 48: ([('ADVP', 48, 48, 8), ('RB', 48, 48, 9)],\n",
       "  [('ADVP', 48, 48, 8), ('RB', 48, 48, 9)]),\n",
       " 49: ([('VP', 49, 67, 8), ('VBZ', 49, 49, 9)], [('VBZ', 49, 49, 9)]),\n",
       " 50: ([('NNP', 50, 50, 11), ('NP', 50, 52, 10), ('NP', 50, 67, 9)],\n",
       "  [('NNP', 50, 50, 11)]),\n",
       " 51: ([('CC', 51, 51, 11)], [('CC', 51, 51, 11)]),\n",
       " 52: ([('NNP', 52, 52, 11)], [('NP', 50, 52, 10), ('NNP', 52, 52, 11)]),\n",
       " 53: ([('VBN', 53, 53, 11), ('VP', 53, 63, 10)], [('VBN', 53, 53, 11)]),\n",
       " 54: ([('NP', 54, 56, 12), ('NNP', 54, 54, 13), ('NP', 54, 58, 11)],\n",
       "  [('NNP', 54, 54, 13)]),\n",
       " 55: ([('NNP', 55, 55, 13)], [('NNP', 55, 55, 13)]),\n",
       " 56: ([('NNP', 56, 56, 13)], [('NNP', 56, 56, 13), ('NP', 54, 56, 12)]),\n",
       " 57: ([('JJ', 57, 57, 13), ('ADJP', 57, 58, 12)], [('JJ', 57, 57, 13)]),\n",
       " 58: ([('JJ', 58, 58, 13)],\n",
       "  [('ADJP', 57, 58, 12), ('JJ', 58, 58, 13), ('NP', 54, 58, 11)]),\n",
       " 59: ([(',', 59, 59, 11)], [(',', 59, 59, 11)]),\n",
       " 60: ([(',', 60, 60, 12), ('FRAG', 60, 63, 11)], [(',', 60, 60, 12)]),\n",
       " 61: ([('IN', 61, 61, 13), ('PP', 61, 62, 12)], [('IN', 61, 61, 13)]),\n",
       " 62: ([('NNP', 62, 62, 14), ('NP', 62, 62, 13)],\n",
       "  [('PP', 61, 62, 12), ('NNP', 62, 62, 14), ('NP', 62, 62, 13)]),\n",
       " 63: ([(',', 63, 63, 12)],\n",
       "  [(',', 63, 63, 12), ('VP', 53, 63, 10), ('FRAG', 60, 63, 11)]),\n",
       " 64: ([('PRN', 64, 67, 10), (',', 64, 64, 11)], [(',', 64, 64, 11)]),\n",
       " 65: ([('RB', 65, 65, 12), ('ADVP', 65, 65, 11)],\n",
       "  [('RB', 65, 65, 12), ('ADVP', 65, 65, 11)]),\n",
       " 66: ([('NP', 66, 66, 11), ('NNP', 66, 66, 12)],\n",
       "  [('NP', 66, 66, 11), ('NNP', 66, 66, 12)]),\n",
       " 67: ([(',', 67, 67, 11)],\n",
       "  [('S', 47, 67, 7),\n",
       "   (',', 67, 67, 11),\n",
       "   ('PRN', 64, 67, 10),\n",
       "   ('VP', 49, 67, 8),\n",
       "   ('NP', 50, 67, 9)]),\n",
       " 68: ([(',', 68, 68, 7)], [(',', 68, 68, 7)]),\n",
       " 69: ([('PRP', 69, 69, 9), ('NP', 69, 69, 8), ('S', 69, 76, 7)],\n",
       "  [('PRP', 69, 69, 9), ('NP', 69, 69, 8)]),\n",
       " 70: ([('ADVP', 70, 70, 8), ('RB', 70, 70, 9)],\n",
       "  [('ADVP', 70, 70, 8), ('RB', 70, 70, 9)]),\n",
       " 71: ([('VBD', 71, 71, 9), ('VP', 71, 76, 8)], [('VBD', 71, 71, 9)]),\n",
       " 72: ([('S', 72, 76, 10),\n",
       "   ('NP', 72, 72, 11),\n",
       "   ('SBAR', 72, 76, 9),\n",
       "   ('NNP', 72, 72, 12)],\n",
       "  [('NP', 72, 72, 11), ('NNP', 72, 72, 12)]),\n",
       " 73: ([('RB', 73, 73, 12), ('ADVP', 73, 73, 11)],\n",
       "  [('RB', 73, 73, 12), ('ADVP', 73, 73, 11)]),\n",
       " 74: ([('VBP', 74, 74, 12), ('VP', 74, 76, 11)], [('VBP', 74, 74, 12)]),\n",
       " 75: ([('JJ', 75, 75, 13), ('NP', 75, 76, 12)], [('JJ', 75, 75, 13)]),\n",
       " 76: ([('NN', 76, 76, 13)],\n",
       "  [('S', 72, 76, 10),\n",
       "   ('VP', 74, 76, 11),\n",
       "   ('SBAR', 47, 76, 5),\n",
       "   ('NN', 76, 76, 13),\n",
       "   ('NP', 75, 76, 12),\n",
       "   ('S', 47, 76, 6),\n",
       "   ('VP', 71, 76, 8),\n",
       "   ('SBAR', 72, 76, 9),\n",
       "   ('S', 69, 76, 7)]),\n",
       " 77: ([('.', 77, 77, 5)],\n",
       "  [('FRAG', 45, 77, 4),\n",
       "   ('VP', 9, 77, 2),\n",
       "   ('.', 77, 77, 5),\n",
       "   ('VP', 11, 77, 3)]),\n",
       " 78: ([('.', 78, 78, 2)], [('.', 78, 78, 2), ('S', 1, 78, 1)])}"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_word_constituents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# next we will compute the top syntactic tag\n",
    "top_tags = []\n",
    "# create a new list (it will contain dictionaries with top syntactic tags)\n",
    "for tree_dict in one_word_constituents:\n",
    "    tags_dict = {}\n",
    "    for key, value in tree_dict.items(): # e.g. 1 --> key, ([(\"NP\", 1, 6), (\"S\", 1, 44)], [(\"NNP\", 1, 1)]) -->value\n",
    "        top_tag_begin = max([(element[2], element[0]) for element in value[0]])[1]\n",
    "        top_tag_end = min([(element[1], element[0]) for element in value[1]])[1]\n",
    "\n",
    "        tags_dict.update({key: (top_tag_begin, top_tag_end)})\n",
    "    top_tags.append(tags_dict)\n",
    "\n",
    "#     Adding the Depth to the list with Top Syntactic Tags\n",
    "top_depths = []\n",
    "for tree_dict in one_word_constituents:\n",
    "    tags_dict = {}\n",
    "    for key, value in tree_dict.items():\n",
    "        top_tag_begin_depth = max([(element[2], element[3]) for element in value[0]])[1]\n",
    "        top_tag_end_depth = min([(element[1], element[3]) for element in value[1]])[1]\n",
    "\n",
    "        tags_dict.update({key: (top_tag_begin_depth, top_tag_end_depth)})\n",
    "    top_depths.append(tags_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(file_name, word2vec_model, subfolder):\n",
    "    # Reading the EDU file\n",
    "    RST_file = open(os.path.join(\"../nlp_project/rst_discourse_treebank/data/RSTtrees-WSJ-main-1.0/\" + subfolder,\n",
    "                                 file_name) + \".edus\")\n",
    "    lines = RST_file.readlines()\n",
    "    RST_file.close()\n",
    "    \n",
    "    # tokenizing the EDUs\n",
    "    tokenized_edus = []\n",
    "    for edu in lines:\n",
    "        tokenized_edu = []\n",
    "        for token in word_tokenize(edu):\n",
    "            if token == \"(\":\n",
    "                tokenized_edu.append(\"OPENING_ROUND_BRACE\")\n",
    "            elif token == \")\":\n",
    "                tokenized_edu.append(\"CLOSING_ROUND_BRACE\")\n",
    "            else:\n",
    "                tokenized_edu.append(token)\n",
    "        tokenized_edus.append(tokenized_edu)\n",
    "        \n",
    "    # Remember boundary indices, combine EDUs of a sentence\n",
    "    boundary_indices = [] \n",
    "    edu_boundary = []\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    for edu in tokenized_edus:\n",
    "        if (edu[-1] not in [\"!\", \"?\", \".\"])\\\n",
    "                and (edu[-2:] not in [\"!\\\"\", \"?\\\"\", \".\\\"\", \"!'\", \"?'\", \".'\"])\\\n",
    "                and (edu[-3:] not in [\"!''\", \"?''\", \".''\"]):\n",
    "            sentence.extend(edu)\n",
    "            edu_boundary.append(len(sentence) - 1)\n",
    "        else:\n",
    "            sentence.extend(edu)\n",
    "            sentences.append(sentence)\n",
    "            edu_boundary.append(len(sentence) - 1)\n",
    "            boundary_indices.append(edu_boundary)\n",
    "            sentence = []\n",
    "            edu_boundary = []\n",
    "            \n",
    "    # getting a POS-tag from nltk\n",
    "    pos_sentences = []\n",
    "    for sentence in sentences:\n",
    "        pos_sentence = nltk.pos_tag(sentence)\n",
    "        pos_sentences.append(pos_sentence)\n",
    "        \n",
    "    # writing a text in conll format\n",
    "#     conll_file = open(\"edu_segmentation/\" + file_name + \".conll\", \"w\")\n",
    "    \n",
    "#     for pos_sentence in pos_sentences:\n",
    "#         for word, pos in pos_sentence:\n",
    "#             conll_file.write(word + \" \" + pos + \"\\n\")\n",
    "#         conll_file.write(\"\\n\")\n",
    "\n",
    "#     conll_file.close()\n",
    "    \n",
    "    # writing the text in a file for syntactic parsing\n",
    "    text_file = open(\"edu_segmentation/\" + file_name + \".text\", \"w\")\n",
    "    text_file.write(\"\\n\".join([\" \".join(sentence) for sentence in sentences]))\n",
    "    text_file.close()\n",
    "\n",
    "    os.environ[\"FILE_NAME\"] = file_name\n",
    "\n",
    "    # performing syntactic parsing\n",
    "    os.system(\"./stanford-parser-full-2017-06-09/lexparser.sh edu_segmentation/\\\"$FILE_NAME\\\".text > edu_segmentation/\\\"$FILE_NAME\\\".penn\")\n",
    "\n",
    "    with open(\"edu_segmentation/\" + file_name + \".penn\") as file:\n",
    "        trees_string = file.read()\n",
    "        trees = trees_string.split(\"\\n\\n\")\n",
    "    \n",
    "    one_word_constituents = []\n",
    "    for tree in trees:\n",
    "        tree_dict = {}\n",
    "        for constituent in constituents(tree):\n",
    "            if constituent[1] not in tree_dict:\n",
    "                tree_dict[constituent[1]] = ([],[])\n",
    "            tree_dict[constituent[1]][0].append(constituent)\n",
    "            if constituent[2] not in tree_dict:\n",
    "                tree_dict[constituent[2]] = ([],[])\n",
    "            second_constituent = tree_dict[constituent[2]][1].append(constituent)\n",
    "        one_word_constituents.append(tree_dict)\n",
    "        \n",
    "    # next we will compute the top syntactic tag\n",
    "    top_tags = []\n",
    "    # create a new list (it will contain dictionaries with top syntactic tags)\n",
    "    for tree_dict in one_word_constituents:\n",
    "        tags_dict = {}\n",
    "        for key, value in tree_dict.items(): # e.g. 1 --> key, ([(\"NP\", 1, 6), (\"S\", 1, 44)], [(\"NNP\", 1, 1)]) -->value\n",
    "            top_tag_begin = max([(element[2], element[0]) for element in value[0]])[1]\n",
    "            top_tag_end = min([(element[1], element[0]) for element in value[1]])[1]\n",
    "\n",
    "            tags_dict.update({key: (top_tag_begin, top_tag_end)})\n",
    "        top_tags.append(tags_dict)\n",
    "    \n",
    "#     Adding the Depth to the list with Top Syntactic Tags\n",
    "    top_depths = []\n",
    "    for tree_dict in one_word_constituents:\n",
    "        tags_dict = {}\n",
    "        for key, value in tree_dict.items():\n",
    "            top_tag_begin_depth = max([(element[2], element[3]) for element in value[0]])[1]\n",
    "            top_tag_end_depth = min([(element[1], element[3]) for element in value[1]])[1]\n",
    "\n",
    "            tags_dict.update({key: (top_tag_begin_depth, top_tag_end_depth)})\n",
    "        top_depths.append(tags_dict)\n",
    "    \n",
    "    #enumerate tokens of the sentence (use *enumerate*) and iterate over them \n",
    "    # (the current token will be the middle left 4-gram element).\n",
    "    data = []\n",
    "    #check if there is a edu break after the token\n",
    "    for (sentence_no, pos_sentence) in enumerate(pos_sentences):\n",
    "        for (token_no, token) in enumerate(pos_sentence): #token_no = key\n",
    "            if token in boundary_indices[sentence_no]: # taking boundary indices of a corresponding sentence\n",
    "                edu_break = True\n",
    "            else:\n",
    "                edu_break = False\n",
    "\n",
    "    # take a top_tag_beg from top_tags and add it to the table with sent_no\n",
    "            top_tag_beg = top_tags[sentence_no][token_no +1][0] #because we've counted them starting from 1\n",
    "            top_tag_end = top_tags[sentence_no][token_no +1][1]\n",
    "            depth_beg = top_depths[sentence_no][token_no +1][0]\n",
    "            depth_end = top_depths[sentence_no][token_no +1][1] \n",
    "\n",
    "    # get a vector\n",
    "            word = token[0]\n",
    "            POS_tag = token[1]\n",
    "\n",
    "    # form a tuple with features which i'll put to the data\n",
    "\n",
    "            feature_vectors = []\n",
    "\n",
    "            feature_vectors.append(file_name)\n",
    "            feature_vectors.append(sentence_no)\n",
    "            feature_vectors.append(token_no)\n",
    "            feature_vectors.append(word)\n",
    "            feature_vectors.append(edu_break)\n",
    "            feature_vectors.append(POS_tag)\n",
    "            feature_vectors.append(top_tag_beg)\n",
    "            feature_vectors.append(top_tag_end)\n",
    "            feature_vectors.append(depth_beg)\n",
    "            feature_vectors.append(depth_end)\n",
    "\n",
    "            if word in model.vocab:\n",
    "                word_vector = model.get_vector(word)\n",
    "                feature_vectors.extend(list(word_vector))\n",
    "            else:\n",
    "                feature_vectors.extend([0]*300)\n",
    "\n",
    "            data.append(feature_vectors)\n",
    "            \n",
    "    df = pd.DataFrame(data, columns=[\"file_name\", \n",
    "                                 \"sentence_no\", \n",
    "                                 \"token_no\", \n",
    "                                 \"word\", \n",
    "                                 \"edu_break\", \n",
    "                                 \"POS-tag\", \n",
    "                                 \"top_tag_beg\", \n",
    "                                 \"top_tag_end\", \n",
    "                                 \"depth_beg\", \n",
    "                                 \"depth_end\"] + [\"c_%.3d\" % i for i in range(300)])\n",
    "    \n",
    "    df.to_csv(\"edu_segmentation/\" + file_name + \".csv\", index=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Google's pre-trained Word2Vec model.\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_file_names(ls_result, target_ext):\n",
    "    file_names = []\n",
    "    \n",
    "    for fn in ls_result:\n",
    "        base_name, ext = os.path.splitext(fn)\n",
    "        if ext == target_ext:\n",
    "            file_names.append(base_name)\n",
    "                \n",
    "    return file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_features(subfolder, file_names):\n",
    "    for file_name in tqdm(file_names, total=len(file_names)):\n",
    "        extract_features(file_name, model, subfolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_csv(subfolder):\n",
    "    ls_result = os.listdir(\"rst_discourse_treebank/data/RSTtrees-WSJ-main-1.0/\" + subfolder)\n",
    "    file_names = get_file_names(ls_result)\n",
    "    compute_features(subfolder, file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_rel_feature(vectorized_table, col_name): # a column in which I'll make changes\n",
    "    vectorized_table.loc[:, \"sent_max_\" + col_name] = vectorized_table.groupby([\"file_name\", \"sentence_no\"]) \\\n",
    "                                                        [col_name].transform(\"max\")\n",
    "    vectorized_table.loc[:, \"rel_\" + col_name] = (vectorized_table[col_name] \\\n",
    "                                / vectorized_table[\"sent_max_\" + col_name] - 0.5) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_table(path, file_names, ohe=True, sent_start_no=0):    \n",
    "    dfs = []\n",
    "    for file in file_names:\n",
    "        df = pd.read_csv(os.path.join(path, file + \".csv\")) # for safety if there's a / at the end of the path\n",
    "        dfs.append(df)\n",
    "\n",
    "    df = pd.concat(dfs)\n",
    "    \n",
    "    if ohe:\n",
    "        # transforming into vector form\n",
    "        vectorized_table = pd.get_dummies(df, \n",
    "                                          columns=[\"POS-tag\", \n",
    "                                                   \"top_tag_beg\", \n",
    "                                                   \"top_tag_end\"])\n",
    "    else:\n",
    "        vectorized_table = df\n",
    "\n",
    "    sent_global_no_df = vectorized_table[[\"file_name\", \"sentence_no\"]].drop_duplicates().sort_values(by=\"file_name\")\n",
    "    sent_global_no_df.loc[:, \"sentence_global_no\"] = range(sent_start_no,\n",
    "                                                           sent_start_no + len(sent_global_no_df))\n",
    "    \n",
    "    vectorized_table = pd.merge(vectorized_table,\n",
    "                                sent_global_no_df,\n",
    "                                on=[\"file_name\", \"sentence_no\"])\n",
    "\n",
    "    vectorized_table.loc[:, \"sent_word_indices\"] = vectorized_table.apply(lambda row: str(row.sentence_global_no) + \"_\" + str(row.token_no),\n",
    "                                                                          axis=1)\n",
    "\n",
    "    vectorized_table = vectorized_table[[\"word\"] + [c for c in vectorized_table.columns if c != \"word\"]]\n",
    "    \n",
    "    vectorized_table.loc[:, \"depth_beg\"] = vectorized_table.depth_beg - 1\n",
    "    vectorized_table.loc[:, \"depth_end\"] = vectorized_table.depth_end - 1\n",
    "    add_rel_feature(vectorized_table, \"depth_beg\")\n",
    "    add_rel_feature(vectorized_table, \"depth_end\")\n",
    "    add_rel_feature(vectorized_table, \"token_no\")\n",
    "\n",
    "    vectorized_table = vectorized_table.drop([\"sentence_no\", \n",
    "                                              \"sentence_global_no\",\n",
    "                                              \"depth_beg\",\n",
    "                                              \"depth_end\",\n",
    "                                              \"token_no\", \n",
    "                                              \"edu_break\",\n",
    "                                              \"sent_max_depth_beg\",\n",
    "                                              \"sent_max_depth_end\",\n",
    "                                              \"sent_max_token_no\"], axis=1)\n",
    "    vectorized_table = vectorized_table.drop([c for c in vectorized_table.columns if c.split(\"_\", 1)[0] == \"c\"],\n",
    "                                             axis=1)\n",
    "\n",
    "    return vectorized_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_ls_result = os.listdir(\"rst_discourse_treebank/data/RSTtrees-WSJ-main-1.0/TRAINING/\")\n",
    "train_file_names = get_file_names(train_ls_result, \".rst\")\n",
    "test_ls_result = os.listdir(\"rst_discourse_treebank/data/RSTtrees-WSJ-main-1.0/TEST/\")\n",
    "test_file_names = get_file_names(test_ls_result, \".rst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a table for Training and Test files\n",
    "# keeping test and training files in one table, dividing it into two tables (so that they have the same # of columns)\n",
    "file_name_df = pd.DataFrame({\"file_name\": train_file_names + test_file_names,\n",
    "                             \"is_train\": [True]*len(train_file_names) + [False]*len(test_file_names)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_table = create_table(\"edu_segmentation/\", \n",
    "                                train_file_names + test_file_names)\n",
    "\n",
    "vectorized_table__split = pd.merge(vectorized_table,\n",
    "                                   file_name_df,\n",
    "                                   on=\"file_name\")\n",
    "\n",
    "train_data_set = vectorized_table__split.loc[vectorized_table__split.is_train == True] \\\n",
    "                                        .drop(\"is_train\", axis=1)\n",
    "test_data_set = vectorized_table__split.loc[vectorized_table__split.is_train == False] \\\n",
    "                                        .drop(\"is_train\", axis=1)\n",
    "\n",
    "train_data_set.to_csv(\"data_set_1__train.csv\", index=False) # 1 - when test and train.files are processed together\n",
    "test_data_set.to_csv(\"data_set_1__test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_table = create_table(\"edu_segmentation/\", train_file_names, False)\n",
    "\n",
    "test_table = create_table(\"edu_segmentation/\", \n",
    "                          test_file_names, \n",
    "                          False,\n",
    "                          1 + max([int(sw.split(\"_\")[0]) \\\n",
    "                                   for sw in train_table.sent_word_indices]))\n",
    "\n",
    "train_table.loc[:, \"is_train\"] = [True] * len(train_table)\n",
    "test_table.loc[:, \"is_train\"] = [False] * len(test_table)\n",
    "\n",
    "table = pd.concat([train_table, test_table])\n",
    "\n",
    "vectorized_table_2 = pd.get_dummies(table,\n",
    "                                    columns=[\"POS-tag\", \n",
    "                                             \"top_tag_beg\", \n",
    "                                             \"top_tag_end\"])\n",
    "\n",
    "vectorized_table_2 = vectorized_table_2[[\"sent_word_indices\",\n",
    "                                         \"word\",\n",
    "                                         \"file_name\"]\n",
    "                                        + [c for c in vectorized_table_2.columns \\\n",
    "                                              if c not in [\"sent_word_indices\",\n",
    "                                                           \"word\",\n",
    "                                                           \"file_name\",\n",
    "                                                           \"rel_depth_beg\",\n",
    "                                                           \"rel_depth_end\",\n",
    "                                                           \"rel_token_no\"]]\n",
    "                                        + [\"rel_depth_beg\",\n",
    "                                           \"rel_depth_end\",\n",
    "                                           \"rel_token_no\"]]\n",
    "\n",
    "train_data_set_2 = vectorized_table_2.loc[vectorized_table_2.is_train == True] \\\n",
    "                                        .drop(\"is_train\", axis=1)\n",
    "test_data_set_2 =  vectorized_table_2.loc[vectorized_table_2.is_train == False] \\\n",
    "                                        .drop(\"is_train\", axis=1)\n",
    "\n",
    "train_data_set_2.to_csv(\"data_set_2__train.csv\", index=False)\n",
    "test_data_set_2.to_csv(\"data_set_2__test.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>file_name</th>\n",
       "      <th>POS-tag_#</th>\n",
       "      <th>POS-tag_$</th>\n",
       "      <th>POS-tag_''</th>\n",
       "      <th>POS-tag_(</th>\n",
       "      <th>POS-tag_)</th>\n",
       "      <th>POS-tag_,</th>\n",
       "      <th>POS-tag_.</th>\n",
       "      <th>POS-tag_:</th>\n",
       "      <th>...</th>\n",
       "      <th>top_tag_end_WHNP</th>\n",
       "      <th>top_tag_end_WHPP</th>\n",
       "      <th>top_tag_end_WP</th>\n",
       "      <th>top_tag_end_WP$</th>\n",
       "      <th>top_tag_end_WRB</th>\n",
       "      <th>top_tag_end_X</th>\n",
       "      <th>top_tag_end_``</th>\n",
       "      <th>rel_depth_beg</th>\n",
       "      <th>rel_depth_end</th>\n",
       "      <th>rel_token_no</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent_word_indices</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0_0</th>\n",
       "      <td>Energetic</td>\n",
       "      <td>file1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_1</th>\n",
       "      <td>and</td>\n",
       "      <td>file1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>-0.947368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_2</th>\n",
       "      <td>concrete</td>\n",
       "      <td>file1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>-0.894737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_3</th>\n",
       "      <td>action</td>\n",
       "      <td>file1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.842105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_4</th>\n",
       "      <td>has</td>\n",
       "      <td>file1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.789474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_5</th>\n",
       "      <td>been</td>\n",
       "      <td>file1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.736842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_6</th>\n",
       "      <td>taken</td>\n",
       "      <td>file1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.684211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_7</th>\n",
       "      <td>in</td>\n",
       "      <td>file1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>-0.631579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_8</th>\n",
       "      <td>Colombia</td>\n",
       "      <td>file1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.75</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>-0.578947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_9</th>\n",
       "      <td>during</td>\n",
       "      <td>file1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>-0.526316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_10</th>\n",
       "      <td>the</td>\n",
       "      <td>file1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.473684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_11</th>\n",
       "      <td>past</td>\n",
       "      <td>file1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.421053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_12</th>\n",
       "      <td>60</td>\n",
       "      <td>file1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-0.368421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_13</th>\n",
       "      <td>days</td>\n",
       "      <td>file1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-0.315789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_14</th>\n",
       "      <td>against</td>\n",
       "      <td>file1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-0.263158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_15</th>\n",
       "      <td>the</td>\n",
       "      <td>file1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_16</th>\n",
       "      <td>mafiosi</td>\n",
       "      <td>file1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>-0.157895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_17</th>\n",
       "      <td>of</td>\n",
       "      <td>file1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-0.105263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_18</th>\n",
       "      <td>the</td>\n",
       "      <td>file1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>-0.052632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_19</th>\n",
       "      <td>drug</td>\n",
       "      <td>file1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_20</th>\n",
       "      <td>trade</td>\n",
       "      <td>file1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.052632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_21</th>\n",
       "      <td>,</td>\n",
       "      <td>file1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.105263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_22</th>\n",
       "      <td>but</td>\n",
       "      <td>file1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.157895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_23</th>\n",
       "      <td>it</td>\n",
       "      <td>file1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.210526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_24</th>\n",
       "      <td>has</td>\n",
       "      <td>file1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.263158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_25</th>\n",
       "      <td>not</td>\n",
       "      <td>file1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.315789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_26</th>\n",
       "      <td>been</td>\n",
       "      <td>file1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.368421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_27</th>\n",
       "      <td>sufficiently</td>\n",
       "      <td>file1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.421053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_28</th>\n",
       "      <td>effective</td>\n",
       "      <td>file1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.473684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_29</th>\n",
       "      <td>,</td>\n",
       "      <td>file1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.526316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_30</th>\n",
       "      <td>because</td>\n",
       "      <td>file1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.578947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_31</th>\n",
       "      <td>,</td>\n",
       "      <td>file1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.631579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_32</th>\n",
       "      <td>unfortunately</td>\n",
       "      <td>file1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.684211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_33</th>\n",
       "      <td>,</td>\n",
       "      <td>file1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.736842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_34</th>\n",
       "      <td>it</td>\n",
       "      <td>file1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.789474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_35</th>\n",
       "      <td>came</td>\n",
       "      <td>file1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.842105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_36</th>\n",
       "      <td>too</td>\n",
       "      <td>file1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.894737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_37</th>\n",
       "      <td>late</td>\n",
       "      <td>file1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.947368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0_38</th>\n",
       "      <td>.</td>\n",
       "      <td>file1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39 rows × 182 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            word file_name  POS-tag_#  POS-tag_$  POS-tag_''  \\\n",
       "sent_word_indices                                                              \n",
       "0_0                    Energetic     file1          0          0           0   \n",
       "0_1                          and     file1          0          0           0   \n",
       "0_2                     concrete     file1          0          0           0   \n",
       "0_3                       action     file1          0          0           0   \n",
       "0_4                          has     file1          0          0           0   \n",
       "0_5                         been     file1          0          0           0   \n",
       "0_6                        taken     file1          0          0           0   \n",
       "0_7                           in     file1          0          0           0   \n",
       "0_8                     Colombia     file1          0          0           0   \n",
       "0_9                       during     file1          0          0           0   \n",
       "0_10                         the     file1          0          0           0   \n",
       "0_11                        past     file1          0          0           0   \n",
       "0_12                          60     file1          0          0           0   \n",
       "0_13                        days     file1          0          0           0   \n",
       "0_14                     against     file1          0          0           0   \n",
       "0_15                         the     file1          0          0           0   \n",
       "0_16                     mafiosi     file1          0          0           0   \n",
       "0_17                          of     file1          0          0           0   \n",
       "0_18                         the     file1          0          0           0   \n",
       "0_19                        drug     file1          0          0           0   \n",
       "0_20                       trade     file1          0          0           0   \n",
       "0_21                           ,     file1          0          0           0   \n",
       "0_22                         but     file1          0          0           0   \n",
       "0_23                          it     file1          0          0           0   \n",
       "0_24                         has     file1          0          0           0   \n",
       "0_25                         not     file1          0          0           0   \n",
       "0_26                        been     file1          0          0           0   \n",
       "0_27                sufficiently     file1          0          0           0   \n",
       "0_28                   effective     file1          0          0           0   \n",
       "0_29                           ,     file1          0          0           0   \n",
       "0_30                     because     file1          0          0           0   \n",
       "0_31                           ,     file1          0          0           0   \n",
       "0_32               unfortunately     file1          0          0           0   \n",
       "0_33                           ,     file1          0          0           0   \n",
       "0_34                          it     file1          0          0           0   \n",
       "0_35                        came     file1          0          0           0   \n",
       "0_36                         too     file1          0          0           0   \n",
       "0_37                        late     file1          0          0           0   \n",
       "0_38                           .     file1          0          0           0   \n",
       "\n",
       "                   POS-tag_(  POS-tag_)  POS-tag_,  POS-tag_.  POS-tag_:  \\\n",
       "sent_word_indices                                                          \n",
       "0_0                        0          0          0          0          0   \n",
       "0_1                        0          0          0          0          0   \n",
       "0_2                        0          0          0          0          0   \n",
       "0_3                        0          0          0          0          0   \n",
       "0_4                        0          0          0          0          0   \n",
       "0_5                        0          0          0          0          0   \n",
       "0_6                        0          0          0          0          0   \n",
       "0_7                        0          0          0          0          0   \n",
       "0_8                        0          0          0          0          0   \n",
       "0_9                        0          0          0          0          0   \n",
       "0_10                       0          0          0          0          0   \n",
       "0_11                       0          0          0          0          0   \n",
       "0_12                       0          0          0          0          0   \n",
       "0_13                       0          0          0          0          0   \n",
       "0_14                       0          0          0          0          0   \n",
       "0_15                       0          0          0          0          0   \n",
       "0_16                       0          0          0          0          0   \n",
       "0_17                       0          0          0          0          0   \n",
       "0_18                       0          0          0          0          0   \n",
       "0_19                       0          0          0          0          0   \n",
       "0_20                       0          0          0          0          0   \n",
       "0_21                       0          0          1          0          0   \n",
       "0_22                       0          0          0          0          0   \n",
       "0_23                       0          0          0          0          0   \n",
       "0_24                       0          0          0          0          0   \n",
       "0_25                       0          0          0          0          0   \n",
       "0_26                       0          0          0          0          0   \n",
       "0_27                       0          0          0          0          0   \n",
       "0_28                       0          0          0          0          0   \n",
       "0_29                       0          0          1          0          0   \n",
       "0_30                       0          0          0          0          0   \n",
       "0_31                       0          0          1          0          0   \n",
       "0_32                       0          0          0          0          0   \n",
       "0_33                       0          0          1          0          0   \n",
       "0_34                       0          0          0          0          0   \n",
       "0_35                       0          0          0          0          0   \n",
       "0_36                       0          0          0          0          0   \n",
       "0_37                       0          0          0          0          0   \n",
       "0_38                       0          0          0          1          0   \n",
       "\n",
       "                       ...       top_tag_end_WHNP  top_tag_end_WHPP  \\\n",
       "sent_word_indices      ...                                            \n",
       "0_0                    ...                      0                 0   \n",
       "0_1                    ...                      0                 0   \n",
       "0_2                    ...                      0                 0   \n",
       "0_3                    ...                      0                 0   \n",
       "0_4                    ...                      0                 0   \n",
       "0_5                    ...                      0                 0   \n",
       "0_6                    ...                      0                 0   \n",
       "0_7                    ...                      0                 0   \n",
       "0_8                    ...                      0                 0   \n",
       "0_9                    ...                      0                 0   \n",
       "0_10                   ...                      0                 0   \n",
       "0_11                   ...                      0                 0   \n",
       "0_12                   ...                      0                 0   \n",
       "0_13                   ...                      0                 0   \n",
       "0_14                   ...                      0                 0   \n",
       "0_15                   ...                      0                 0   \n",
       "0_16                   ...                      0                 0   \n",
       "0_17                   ...                      0                 0   \n",
       "0_18                   ...                      0                 0   \n",
       "0_19                   ...                      0                 0   \n",
       "0_20                   ...                      0                 0   \n",
       "0_21                   ...                      0                 0   \n",
       "0_22                   ...                      0                 0   \n",
       "0_23                   ...                      0                 0   \n",
       "0_24                   ...                      0                 0   \n",
       "0_25                   ...                      0                 0   \n",
       "0_26                   ...                      0                 0   \n",
       "0_27                   ...                      0                 0   \n",
       "0_28                   ...                      0                 0   \n",
       "0_29                   ...                      0                 0   \n",
       "0_30                   ...                      0                 0   \n",
       "0_31                   ...                      0                 0   \n",
       "0_32                   ...                      0                 0   \n",
       "0_33                   ...                      0                 0   \n",
       "0_34                   ...                      0                 0   \n",
       "0_35                   ...                      0                 0   \n",
       "0_36                   ...                      0                 0   \n",
       "0_37                   ...                      0                 0   \n",
       "0_38                   ...                      0                 0   \n",
       "\n",
       "                   top_tag_end_WP  top_tag_end_WP$  top_tag_end_WRB  \\\n",
       "sent_word_indices                                                     \n",
       "0_0                             0                0                0   \n",
       "0_1                             0                0                0   \n",
       "0_2                             0                0                0   \n",
       "0_3                             0                0                0   \n",
       "0_4                             0                0                0   \n",
       "0_5                             0                0                0   \n",
       "0_6                             0                0                0   \n",
       "0_7                             0                0                0   \n",
       "0_8                             0                0                0   \n",
       "0_9                             0                0                0   \n",
       "0_10                            0                0                0   \n",
       "0_11                            0                0                0   \n",
       "0_12                            0                0                0   \n",
       "0_13                            0                0                0   \n",
       "0_14                            0                0                0   \n",
       "0_15                            0                0                0   \n",
       "0_16                            0                0                0   \n",
       "0_17                            0                0                0   \n",
       "0_18                            0                0                0   \n",
       "0_19                            0                0                0   \n",
       "0_20                            0                0                0   \n",
       "0_21                            0                0                0   \n",
       "0_22                            0                0                0   \n",
       "0_23                            0                0                0   \n",
       "0_24                            0                0                0   \n",
       "0_25                            0                0                0   \n",
       "0_26                            0                0                0   \n",
       "0_27                            0                0                0   \n",
       "0_28                            0                0                0   \n",
       "0_29                            0                0                0   \n",
       "0_30                            0                0                0   \n",
       "0_31                            0                0                0   \n",
       "0_32                            0                0                0   \n",
       "0_33                            0                0                0   \n",
       "0_34                            0                0                0   \n",
       "0_35                            0                0                0   \n",
       "0_36                            0                0                0   \n",
       "0_37                            0                0                0   \n",
       "0_38                            0                0                0   \n",
       "\n",
       "                   top_tag_end_X  top_tag_end_``  rel_depth_beg  \\\n",
       "sent_word_indices                                                 \n",
       "0_0                            0               0          -1.00   \n",
       "0_1                            0               0          -0.25   \n",
       "0_2                            0               0          -0.50   \n",
       "0_3                            0               0          -0.50   \n",
       "0_4                            0               0          -0.25   \n",
       "0_5                            0               0          -0.25   \n",
       "0_6                            0               0          -0.25   \n",
       "0_7                            0               0          -0.25   \n",
       "0_8                            0               0          -0.75   \n",
       "0_9                            0               0          -0.25   \n",
       "0_10                           0               0          -0.50   \n",
       "0_11                           0               0          -0.25   \n",
       "0_12                           0               0           0.00   \n",
       "0_13                           0               0           0.25   \n",
       "0_14                           0               0           0.50   \n",
       "0_15                           0               0           0.50   \n",
       "0_16                           0               0           0.50   \n",
       "0_17                           0               0           0.75   \n",
       "0_18                           0               0           0.25   \n",
       "0_19                           0               0           0.50   \n",
       "0_20                           0               0           0.50   \n",
       "0_21                           0               0           0.75   \n",
       "0_22                           0               0           0.75   \n",
       "0_23                           0               0           0.75   \n",
       "0_24                           0               0          -0.25   \n",
       "0_25                           0               0           0.25   \n",
       "0_26                           0               0           0.25   \n",
       "0_27                           0               0           0.00   \n",
       "0_28                           0               0           0.25   \n",
       "0_29                           0               0           0.25   \n",
       "0_30                           0               0           0.25   \n",
       "0_31                           0               0           0.50   \n",
       "0_32                           0               0           0.50   \n",
       "0_33                           0               0           0.50   \n",
       "0_34                           0               0           0.00   \n",
       "0_35                           0               0           0.50   \n",
       "0_36                           0               0           0.75   \n",
       "0_37                           0               0           1.00   \n",
       "0_38                           0               0           1.00   \n",
       "\n",
       "                   rel_depth_end  rel_token_no  \n",
       "sent_word_indices                               \n",
       "0_0                        -0.25     -1.000000  \n",
       "0_1                        -0.50     -0.947368  \n",
       "0_2                        -0.50     -0.894737  \n",
       "0_3                        -0.25     -0.842105  \n",
       "0_4                        -0.25     -0.789474  \n",
       "0_5                        -0.25     -0.736842  \n",
       "0_6                        -0.25     -0.684211  \n",
       "0_7                        -0.75     -0.631579  \n",
       "0_8                        -0.50     -0.578947  \n",
       "0_9                        -0.50     -0.526316  \n",
       "0_10                       -0.25     -0.473684  \n",
       "0_11                        0.00     -0.421053  \n",
       "0_12                        0.25     -0.368421  \n",
       "0_13                        0.50     -0.315789  \n",
       "0_14                        0.50     -0.263158  \n",
       "0_15                        0.50     -0.210526  \n",
       "0_16                        0.75     -0.157895  \n",
       "0_17                        0.25     -0.105263  \n",
       "0_18                        0.50     -0.052632  \n",
       "0_19                        0.50      0.000000  \n",
       "0_20                        0.75      0.052632  \n",
       "0_21                        0.75      0.105263  \n",
       "0_22                        0.75      0.157895  \n",
       "0_23                       -0.25      0.210526  \n",
       "0_24                        0.25      0.263158  \n",
       "0_25                        0.25      0.315789  \n",
       "0_26                        0.00      0.368421  \n",
       "0_27                        0.25      0.421053  \n",
       "0_28                        0.25      0.473684  \n",
       "0_29                        0.25      0.526316  \n",
       "0_30                        0.50      0.578947  \n",
       "0_31                        0.50      0.631579  \n",
       "0_32                        0.50      0.684211  \n",
       "0_33                        0.00      0.736842  \n",
       "0_34                        0.50      0.789474  \n",
       "0_35                        0.75      0.842105  \n",
       "0_36                        1.00      0.894737  \n",
       "0_37                        1.00      0.947368  \n",
       "0_38                        1.00      1.000000  \n",
       "\n",
       "[39 rows x 182 columns]"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"data_set_1__train.csv\").set_index(\"sent_word_indices\").head(39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ls_result = os.listdir(\"edu_segmentation/\")\n",
    "text_file_names = get_file_names(ls_result, \".text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_names = train_file_names + test_file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_file_names = list(set(file_names).difference(set(text_file_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/194 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Exxon\n",
      "2 Corp.\n",
      "3 is\n",
      "4 resigning\n",
      "5 from\n",
      "6 the\n",
      "7 National\n",
      "8 Wildlife\n",
      "9 Federation\n",
      "10 's\n",
      "11 corporate\n",
      "12 advisory\n",
      "13 panel\n",
      "14 ,\n",
      "15 saying\n",
      "16 the\n",
      "17 conservation\n",
      "18 group\n",
      "19 has\n",
      "20 been\n",
      "21 unfairly\n",
      "22 critical\n",
      "23 of\n",
      "24 the\n",
      "25 Exxon\n",
      "26 Valdez\n",
      "27 oil\n",
      "28 spill\n",
      "29 along\n",
      "30 the\n",
      "31 Alaskan\n",
      "32 coast\n",
      "33 .\n",
      "1 The\n",
      "2 federation\n",
      "3 said\n",
      "4 Friday\n",
      "5 that\n",
      "6 it\n",
      "7 regrets\n",
      "8 the\n",
      "9 resignation\n",
      "10 ,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 2/194 [00:29<46:57, 14.67s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "11 but\n",
      "12 issued\n",
      "13 a\n",
      "14 stinging\n",
      "15 response\n",
      "16 that\n",
      "17 called\n",
      "18 Exxon\n",
      "19 a\n",
      "20 ``\n",
      "21 corporate\n",
      "22 pariah\n",
      "23 ''\n",
      "24 that\n",
      "25 should\n",
      "26 keep\n",
      "27 an\n",
      "28 open\n",
      "29 dialogue\n",
      "30 with\n",
      "31 environmentalists\n",
      "32 .\n",
      "1 The\n",
      "2 federation\n",
      "3 ,\n",
      "4 with\n",
      "5 5.8\n",
      "6 million\n",
      "7 members\n",
      "8 nationwide\n",
      "9 ,\n",
      "10 has\n",
      "11 been\n",
      "12 one\n",
      "13 of\n",
      "14 the\n",
      "15 sharpest\n",
      "16 critics\n",
      "17 of\n",
      "18 Exxon\n",
      "19 's\n",
      "20 handling\n",
      "21 of\n",
      "22 the\n",
      "23 11\n",
      "24 million\n",
      "25 gallon\n",
      "26 tanker\n",
      "27 spill\n",
      "28 and\n",
      "29 has\n",
      "30 accused\n",
      "31 the\n",
      "32 company\n",
      "33 of\n",
      "34 repeatedly\n",
      "35 ignoring\n",
      "36 requests\n",
      "37 to\n",
      "38 meet\n",
      "39 and\n",
      "40 discuss\n",
      "41 it\n",
      "42 .\n",
      "1 The\n",
      "2 March\n",
      "3 24\n",
      "4 oil\n",
      "5 spill\n",
      "6 soiled\n",
      "7 hundreds\n",
      "8 of\n",
      "9 miles\n",
      "10 of\n",
      "11 shoreline\n",
      "12 along\n",
      "13 Alaska\n",
      "14 's\n",
      "15 southern\n",
      "16 coast\n",
      "17 and\n",
      "18 wreaked\n",
      "19 havoc\n",
      "20 with\n",
      "21 wildlife\n",
      "22 and\n",
      "23 the\n",
      "24 fishing\n",
      "25 industry\n",
      "26 .\n",
      "1 Exxon\n",
      "2 's\n",
      "3 Exxon\n",
      "4 USA\n",
      "5 unit\n",
      "6 was\n",
      "7 one\n",
      "8 of\n",
      "9 the\n",
      "10 charter\n",
      "11 members\n",
      "12 of\n",
      "13 the\n",
      "14 Corporate\n",
      "15 Conservation\n",
      "16 Council\n",
      "17 ,\n",
      "18 a\n",
      "19 panel\n",
      "20 of\n",
      "21 executives\n",
      "22 formed\n",
      "23 in\n",
      "24 1982\n",
      "25 by\n",
      "26 the\n",
      "27 National\n",
      "28 Wildlife\n",
      "29 Federation\n",
      "30 to\n",
      "31 foster\n",
      "32 ``\n",
      "33 frank\n",
      "34 and\n",
      "35 open\n",
      "36 discussions\n",
      "37 ''\n",
      "38 between\n",
      "39 industry\n",
      "40 and\n",
      "41 the\n",
      "42 federation\n",
      "43 's\n",
      "44 leaders\n",
      "45 .\n",
      "1 In\n",
      "2 a\n",
      "3 letter\n",
      "4 to\n",
      "5 the\n",
      "6 federation\n",
      "7 ,\n",
      "8 Raymond\n",
      "9 Campion\n",
      "10 ,\n",
      "11 Exxon\n",
      "12 's\n",
      "13 environmental\n",
      "14 coordinator\n",
      "15 ,\n",
      "16 said\n",
      "17 :\n",
      "18 ``\n",
      "19 Recent\n",
      "20 public\n",
      "21 actions\n",
      "22 by\n",
      "23 you\n",
      "24 regarding\n",
      "25 the\n",
      "26 Valdez\n",
      "27 oil\n",
      "28 spill\n",
      "29 have\n",
      "30 failed\n",
      "31 to\n",
      "32 demonstrate\n",
      "33 any\n",
      "34 sense\n",
      "35 of\n",
      "36 objectivity\n",
      "37 or\n",
      "38 fairness\n",
      "39 .\n",
      "40 ''\n",
      "41 The\n",
      "42 federation\n",
      "43 was\n",
      "44 among\n",
      "45 the\n",
      "46 plaintiffs\n",
      "47 in\n",
      "48 a\n",
      "49 lawsuit\n",
      "50 filed\n",
      "51 in\n",
      "52 August\n",
      "53 against\n",
      "54 Exxon\n",
      "55 seeking\n",
      "56 full\n",
      "57 payment\n",
      "58 of\n",
      "59 environmental\n",
      "60 recovery\n",
      "61 costs\n",
      "62 from\n",
      "63 the\n",
      "64 spill\n",
      "65 .\n",
      "1 ABB\n",
      "2 Asea\n",
      "3 Brown\n",
      "4 Boveri\n",
      "5 B.V.\n",
      "6 said\n",
      "7 it\n",
      "8 signed\n",
      "9 a\n",
      "10 contract\n",
      "11 for\n",
      "12 the\n",
      "13 largest-ever\n",
      "14 power\n",
      "15 plant\n",
      "16 order\n",
      "17 in\n",
      "18 the\n",
      "19 Netherlands\n",
      "20 .\n",
      "1 ABB\n",
      "2 said\n",
      "3 the\n",
      "4 contract\n",
      "5 ,\n",
      "6 signed\n",
      "7 with\n",
      "8 the\n",
      "9 Dutch\n",
      "10 utility\n",
      "11 N.V.\n",
      "12 Energieproduktiebedrijf\n",
      "13 UNA\n",
      "14 ,\n",
      "15 is\n",
      "16 valued\n",
      "17 in\n",
      "18 excess\n",
      "19 of\n",
      "20 $\n",
      "21 200\n",
      "22 million\n",
      "23 .\n",
      "1 The\n",
      "2 accord\n",
      "3 is\n",
      "4 for\n",
      "5 a\n",
      "6 turbogenerator\n",
      "7 plant\n",
      "8 at\n",
      "9 the\n",
      "10 coal-fired\n",
      "11 power\n",
      "12 station\n",
      "13 Hemweg\n",
      "14 in\n",
      "15 Amsterdam\n",
      "16 .\n",
      "1 ABB\n",
      "2 Asea\n",
      "3 Brown\n",
      "4 Boveri\n",
      "5 is\n",
      "6 the\n",
      "7 Dutch\n",
      "8 unit\n",
      "9 of\n",
      "10 the\n",
      "11 Swedish-Swiss\n",
      "12 electrical\n",
      "13 engineering\n",
      "14 group\n",
      "15 ABB\n",
      "16 Asea\n",
      "17 Brown\n",
      "18 Boveri\n",
      "19 AG\n",
      "20 .\n",
      "1 ABB\n",
      "2 said\n",
      "3 a\n",
      "4 significant\n",
      "5 portion\n",
      "6 of\n",
      "7 the\n",
      "8 order\n",
      "9 will\n",
      "10 be\n",
      "11 placed\n",
      "12 with\n",
      "13 Dutch\n",
      "14 subcontractors\n",
      "15 ,\n",
      "16 adding\n",
      "17 that\n",
      "18 a\n",
      "19 group\n",
      "20 has\n",
      "21 been\n",
      "22 set\n",
      "23 up\n",
      "24 for\n",
      "25 this\n",
      "26 purpose\n",
      "27 .\n",
      "1 The\n",
      "2 Dutch\n",
      "3 utility\n",
      "4 firm\n",
      "5 serves\n",
      "6 the\n",
      "7 Amsterdam\n",
      "8 and\n",
      "9 Utrecht\n",
      "10 areas\n",
      "11 .\n",
      "1 The\n",
      "2 planned\n",
      "3 turbogenerator\n",
      "4 plant\n",
      "5 is\n",
      "6 expected\n",
      "7 to\n",
      "8 go\n",
      "9 into\n",
      "10 operation\n",
      "11 in\n",
      "12 1994\n",
      "13 .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▏         | 3/194 [00:37<39:48, 12.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Falcon\n",
      "2 Holding\n",
      "3 Group\n",
      "4 Inc.\n",
      "5 said\n",
      "6 it\n",
      "7 agreed\n",
      "8 to\n",
      "9 acquire\n",
      "10 about\n",
      "11 54,000\n",
      "12 subscribers\n",
      "13 from\n",
      "14 First\n",
      "15 Carolina\n",
      "16 Cable\n",
      "17 TV\n",
      "18 Limited\n",
      "19 Partnership\n",
      "20 for\n",
      "21 about\n",
      "22 $\n",
      "23 100\n",
      "24 million\n",
      "25 ,\n",
      "26 or\n",
      "27 roughly\n",
      "28 $\n",
      "29 2,000\n",
      "30 a\n",
      "31 subscriber\n",
      "32 .\n",
      "1 The\n",
      "2 subscribers\n",
      "3 are\n",
      "4 in\n",
      "5 52\n",
      "6 different\n",
      "7 communities\n",
      "8 in\n",
      "9 Georgia\n",
      "10 ,\n",
      "11 Alabama\n",
      "12 and\n",
      "13 Mississippi\n",
      "14 .\n",
      "1 Completion\n",
      "2 of\n",
      "3 the\n",
      "4 sale\n",
      "5 is\n",
      "6 expected\n",
      "7 early\n",
      "8 next\n",
      "9 year\n",
      "10 ,\n",
      "11 Falcon\n",
      "12 said\n",
      "13 .\n",
      "1 Currently\n",
      "2 ,\n",
      "3 Falcon\n",
      "4 has\n",
      "5 about\n",
      "6 750,000\n",
      "7 cable-television\n",
      "8 subscribers\n",
      "9 around\n",
      "10 the\n",
      "11 nation\n",
      "12 ;\n",
      "13 the\n",
      "14 company\n",
      "15 's\n",
      "16 cable-television\n",
      "17 unit\n",
      "18 reported\n",
      "19 1988\n",
      "20 revenue\n",
      "21 of\n",
      "22 about\n",
      "23 $\n",
      "24 100\n",
      "25 million\n",
      "26 .\n",
      "1 In\n",
      "2 composite\n",
      "3 trading\n",
      "4 on\n",
      "5 the\n",
      "6 American\n",
      "7 Stock\n",
      "8 Exchange\n",
      "9 ,\n",
      "10 Falcon\n",
      "11 closed\n",
      "12 at\n",
      "13 $\n",
      "14 20\n",
      "15 ,\n",
      "16 unchanged\n",
      "17 .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  2%|▏         | 4/194 [00:46<36:31, 11.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 David\n",
      "2 Shaffer\n",
      "3 was\n",
      "4 named\n",
      "5 to\n",
      "6 the\n",
      "7 new\n",
      "8 post\n",
      "9 of\n",
      "10 executive\n",
      "11 vice\n",
      "12 president\n",
      "13 of\n",
      "14 the\n",
      "15 Maxwell\n",
      "16 Macmillan\n",
      "17 group\n",
      "18 of\n",
      "19 this\n",
      "20 communications\n",
      "21 giant\n",
      "22 .\n",
      "1 Mr.\n",
      "2 Shaffer\n",
      "3 takes\n",
      "4 primary\n",
      "5 responsibility\n",
      "6 for\n",
      "7 the\n",
      "8 electronic\n",
      "9 and\n",
      "10 technical-services\n",
      "11 group\n",
      "12 .\n",
      "1 He\n",
      "2 had\n",
      "3 been\n",
      "4 group\n",
      "5 vice\n",
      "6 president\n",
      "7 of\n",
      "8 the\n",
      "9 electronic-publishing\n",
      "10 group\n",
      "11 .\n",
      "1 Also\n",
      "2 ,\n",
      "3 Sheldon\n",
      "4 Aboff\n",
      "5 ,\n",
      "6 formerly\n",
      "7 a\n",
      "8 vice\n",
      "9 president\n",
      "10 at\n",
      "11 Maxwell\n",
      "12 ,\n",
      "13 was\n",
      "14 named\n",
      "15 group\n",
      "16 vice\n",
      "17 president\n",
      "18 with\n",
      "19 responsibility\n",
      "20 for\n",
      "21 various\n",
      "22 electronic\n",
      "23 and\n",
      "24 publishing-group\n",
      "25 companies\n",
      "26 .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|▎         | 5/194 [00:53<33:57, 10.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Consumer\n",
      "2 spending\n",
      "3 in\n",
      "4 Britain\n",
      "5 rose\n",
      "6 0.1\n",
      "7 %\n",
      "8 in\n",
      "9 the\n",
      "10 third\n",
      "11 quarter\n",
      "12 from\n",
      "13 the\n",
      "14 second\n",
      "15 quarter\n",
      "16 and\n",
      "17 was\n",
      "18 up\n",
      "19 3.8\n",
      "20 %\n",
      "21 from\n",
      "22 a\n",
      "23 year\n",
      "24 ago\n",
      "25 ,\n",
      "26 the\n",
      "27 Central\n",
      "28 Statistical\n",
      "29 Office\n",
      "30 estimated\n",
      "31 Friday\n",
      "32 .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  3%|▎         | 6/194 [01:04<33:55, 10.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ALBERTA\n",
      "2 ENERGY\n",
      "3 Co.\n",
      "4 ,\n",
      "5 Calgary\n",
      "6 ,\n",
      "7 said\n",
      "8 it\n",
      "9 filed\n",
      "10 a\n",
      "11 preliminary\n",
      "12 prospectus\n",
      "13 for\n",
      "14 an\n",
      "15 offering\n",
      "16 of\n",
      "17 common\n",
      "18 shares\n",
      "19 .\n",
      "1 The\n",
      "2 natural\n",
      "3 resources\n",
      "4 development\n",
      "5 concern\n",
      "6 said\n",
      "7 proceeds\n",
      "8 will\n",
      "9 be\n",
      "10 used\n",
      "11 to\n",
      "12 repay\n",
      "13 long-term\n",
      "14 debt\n",
      "15 ,\n",
      "16 which\n",
      "17 stood\n",
      "18 at\n",
      "19 598\n",
      "20 million\n",
      "21 Canadian\n",
      "22 dollars\n",
      "23 OPENING_ROUND_BRACE\n",
      "24 US\n",
      "25 $\n",
      "26 510.6\n",
      "27 million\n",
      "28 CLOSING_ROUND_BRACE\n",
      "29 at\n",
      "30 the\n",
      "31 end\n",
      "32 of\n",
      "33 1988\n",
      "34 .\n",
      "1 The\n",
      "2 company\n",
      "3 plans\n",
      "4 to\n",
      "5 raise\n",
      "6 between\n",
      "7 C\n",
      "8 $\n",
      "9 75\n",
      "10 million\n",
      "11 and\n",
      "12 C\n",
      "13 $\n",
      "14 100\n",
      "15 million\n",
      "16 from\n",
      "17 the\n",
      "18 offering\n",
      "19 ,\n",
      "20 according\n",
      "21 to\n",
      "22 a\n",
      "23 spokeswoman\n",
      "24 at\n",
      "25 Richardson\n",
      "26 Greenshields\n",
      "27 of\n",
      "28 Canada\n",
      "29 Ltd.\n",
      "30 ,\n",
      "31 lead\n",
      "32 underwriter\n",
      "33 .\n",
      "1 The\n",
      "2 shares\n",
      "3 will\n",
      "4 be\n",
      "5 priced\n",
      "6 in\n",
      "7 early\n",
      "8 November\n",
      "9 ,\n",
      "10 she\n",
      "11 said\n",
      "12 .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▎         | 7/194 [01:12<32:17, 10.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 PARKER\n",
      "2 HANNIFIN\n",
      "3 Corp.\n",
      "4 ,\n",
      "5 which\n",
      "6 is\n",
      "7 selling\n",
      "8 three\n",
      "9 automotive\n",
      "10 replacement\n",
      "11 parts\n",
      "12 divisions\n",
      "13 ,\n",
      "14 said\n",
      "15 it\n",
      "16 will\n",
      "17 retain\n",
      "18 its\n",
      "19 Automotive\n",
      "20 Connectors\n",
      "21 and\n",
      "22 Cliff\n",
      "23 Impact\n",
      "24 divisions\n",
      "25 .\n",
      "1 The\n",
      "2 divisions\n",
      "3 that\n",
      "4 Parker\n",
      "5 Hannifin\n",
      "6 is\n",
      "7 retaining\n",
      "8 were\n",
      "9 n't\n",
      "10 mentioned\n",
      "11 in\n",
      "12 Thursday\n",
      "13 's\n",
      "14 edition\n",
      "15 .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  4%|▍         | 8/194 [01:20<31:06, 10.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 The\n",
      "2 European\n",
      "3 Community\n",
      "4 's\n",
      "5 consumer\n",
      "6 price\n",
      "7 index\n",
      "8 rose\n",
      "9 a\n",
      "10 provisional\n",
      "11 0.6\n",
      "12 %\n",
      "13 in\n",
      "14 September\n",
      "15 from\n",
      "16 August\n",
      "17 and\n",
      "18 was\n",
      "19 up\n",
      "20 5.3\n",
      "21 %\n",
      "22 from\n",
      "23 September\n",
      "24 1988\n",
      "25 ,\n",
      "26 according\n",
      "27 to\n",
      "28 Eurostat\n",
      "29 ,\n",
      "30 the\n",
      "31 EC\n",
      "32 's\n",
      "33 statistical\n",
      "34 agency\n",
      "35 .\n",
      "1 The\n",
      "2 month-to-month\n",
      "3 rise\n",
      "4 in\n",
      "5 the\n",
      "6 index\n",
      "7 was\n",
      "8 the\n",
      "9 largest\n",
      "10 since\n",
      "11 April\n",
      "12 ,\n",
      "13 Eurostat\n",
      "14 said\n",
      "15 .\n",
      "1 DPC\n",
      "2 Acquisition\n",
      "3 Partners\n",
      "4 ,\n",
      "5 a\n",
      "6 hostile\n",
      "7 suitor\n",
      "8 for\n",
      "9 Dataproducts\n",
      "10 Corp.\n",
      "11 ,\n",
      "12 filed\n",
      "13 a\n",
      "14 petition\n",
      "15 in\n",
      "16 federal\n",
      "17 district\n",
      "18 court\n",
      "19 in\n",
      "20 Los\n",
      "21 Angeles\n",
      "22 seeking\n",
      "23 to\n",
      "24 have\n",
      "25 its\n",
      "26 standstill\n",
      "27 agreement\n",
      "28 with\n",
      "29 the\n",
      "30 computer\n",
      "31 printer\n",
      "32 maker\n",
      "33 declared\n",
      "34 void\n",
      "35 ,\n",
      "36 and\n",
      "37 it\n",
      "38 proceeded\n",
      "39 with\n",
      "40 a\n",
      "41 $\n",
      "42 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▍         | 9/194 [01:34<32:25, 10.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10-a-share\n",
      "43 tender\n",
      "44 offer\n",
      "45 for\n",
      "46 the\n",
      "47 company\n",
      "48 .\n",
      "1 The\n",
      "2 offer\n",
      "3 would\n",
      "4 give\n",
      "5 the\n",
      "6 transaction\n",
      "7 an\n",
      "8 indicated\n",
      "9 value\n",
      "10 of\n",
      "11 $\n",
      "12 189\n",
      "13 million\n",
      "14 ,\n",
      "15 based\n",
      "16 on\n",
      "17 the\n",
      "18 18.9\n",
      "19 million\n",
      "20 shares\n",
      "21 the\n",
      "22 group\n",
      "23 does\n",
      "24 n't\n",
      "25 already\n",
      "26 own\n",
      "27 .\n",
      "1 DPC\n",
      "2 holds\n",
      "3 about\n",
      "4 7.8\n",
      "5 %\n",
      "6 of\n",
      "7 Dataproducts\n",
      "8 '\n",
      "9 shares\n",
      "10 .\n",
      "1 Lawyers\n",
      "2 representing\n",
      "3 DPC\n",
      "4 declined\n",
      "5 to\n",
      "6 elaborate\n",
      "7 ,\n",
      "8 saying\n",
      "9 they\n",
      "10 did\n",
      "11 n't\n",
      "12 have\n",
      "13 a\n",
      "14 final\n",
      "15 copy\n",
      "16 of\n",
      "17 the\n",
      "18 filing\n",
      "19 .\n",
      "1 Jack\n",
      "2 Davis\n",
      "3 ,\n",
      "4 Dataproducts\n",
      "5 '\n",
      "6 chairman\n",
      "7 ,\n",
      "8 said\n",
      "9 he\n",
      "10 had\n",
      "11 n't\n",
      "12 yet\n",
      "13 seen\n",
      "14 the\n",
      "15 filing\n",
      "16 and\n",
      "17 could\n",
      "18 n't\n",
      "19 comment\n",
      "20 .\n",
      "1 DPC\n",
      "2 made\n",
      "3 a\n",
      "4 $\n",
      "5 15-a-share\n",
      "6 bid\n",
      "7 for\n",
      "8 the\n",
      "9 company\n",
      "10 in\n",
      "11 May\n",
      "12 ,\n",
      "13 but\n",
      "14 Dataproducts\n",
      "15 management\n",
      "16 considered\n",
      "17 the\n",
      "18 $\n",
      "19 283.7\n",
      "20 million\n",
      "21 proposal\n",
      "22 unacceptable\n",
      "23 .\n",
      "1 Dataproducts\n",
      "2 had\n",
      "3 sought\n",
      "4 a\n",
      "5 buyer\n",
      "6 for\n",
      "7 several\n",
      "8 months\n",
      "9 ,\n",
      "10 but\n",
      "11 it\n",
      "12 is\n",
      "13 now\n",
      "14 in\n",
      "15 the\n",
      "16 midst\n",
      "17 of\n",
      "18 a\n",
      "19 restructuring\n",
      "20 plan\n",
      "21 and\n",
      "22 management\n",
      "23 says\n",
      "24 the\n",
      "25 company\n",
      "26 is\n",
      "27 no\n",
      "28 longer\n",
      "29 for\n",
      "30 sale\n",
      "31 .\n",
      "1 Sheraton\n",
      "2 Corp.\n",
      "3 and\n",
      "4 Pan\n",
      "5 American\n",
      "6 World\n",
      "7 Airways\n",
      "8 announced\n",
      "9 that\n",
      "10 they\n",
      "11 and\n",
      "12 two\n",
      "13 Soviet\n",
      "14 partners\n",
      "15 will\n",
      "16 construct\n",
      "17 two\n",
      "18 ``\n",
      "19 world-class\n",
      "20 ''\n",
      "21 hotels\n",
      "22 within\n",
      "23 a\n",
      "24 mile\n",
      "25 of\n",
      "26 Red\n",
      "27 Square\n",
      "28 in\n",
      "29 Moscow\n",
      "30 .\n",
      "1 U.S.\n",
      "2 and\n",
      "3 Soviet\n",
      "4 officials\n",
      "5 hailed\n",
      "6 the\n",
      "7 joint\n",
      "8 project\n",
      "9 as\n",
      "10 a\n",
      "11 new\n",
      "12 indication\n",
      "13 of\n",
      "14 the\n",
      "15 further\n",
      "16 thaw\n",
      "17 in\n",
      "18 U.S.-Soviet\n",
      "19 relations\n",
      "20 .\n",
      "1 ``\n",
      "2 This\n",
      "3 is\n",
      "4 an\n",
      "5 outstanding\n",
      "6 example\n",
      "7 of\n",
      "8 how\n",
      "9 the\n",
      "10 East\n",
      "11 and\n",
      "12 the\n",
      "13 West\n",
      "14 can\n",
      "15 work\n",
      "16 together\n",
      "17 for\n",
      "18 their\n",
      "19 mutual\n",
      "20 benefit\n",
      "21 and\n",
      "22 progress\n",
      "23 ,\n",
      "24 ''\n",
      "25 said\n",
      "26 Soviet\n",
      "27 Ambassador\n",
      "28 Yuri\n",
      "29 Dubinin\n",
      "30 ,\n",
      "31 who\n",
      "32 hosted\n",
      "33 a\n",
      "34 signing\n",
      "35 ceremony\n",
      "36 for\n",
      "37 the\n",
      "38 venture\n",
      "39 's\n",
      "40 partners\n",
      "41 at\n",
      "42 the\n",
      "43 Soviet\n",
      "44 embassy\n",
      "45 here\n",
      "46 .\n",
      "1 Commerce\n",
      "2 Secretary\n",
      "3 Robert\n",
      "4 Mosbacher\n",
      "5 ,\n",
      "6 who\n",
      "7 attended\n",
      "8 the\n",
      "9 ceremony\n",
      "10 ,\n",
      "11 called\n",
      "12 the\n",
      "13 undertaking\n",
      "14 a\n",
      "15 ``\n",
      "16 historic\n",
      "17 step\n",
      "18 ''\n",
      "19 in\n",
      "20 the\n",
      "21 evolution\n",
      "22 of\n",
      "23 U.S.-Soviet\n",
      "24 ties\n",
      "25 .\n",
      "1 He\n",
      "2 added\n",
      "3 that\n",
      "4 it\n",
      "5 likely\n",
      "6 will\n",
      "7 have\n",
      "8 a\n",
      "9 ``\n",
      "10 mulitiplier\n",
      "11 effect\n",
      "12 ''\n",
      "13 in\n",
      "14 stimulating\n",
      "15 further\n",
      "16 trade\n",
      "17 between\n",
      "18 the\n",
      "19 two\n",
      "20 countries\n",
      "21 .\n",
      "1 The\n",
      "2 project\n",
      "3 will\n",
      "4 be\n",
      "5 the\n",
      "6 largest\n",
      "7 U.S.-backed\n",
      "8 joint\n",
      "9 venture\n",
      "10 to\n",
      "11 be\n",
      "12 undertaken\n",
      "13 in\n",
      "14 the\n",
      "15 Soviet\n",
      "16 Union\n",
      "17 in\n",
      "18 recent\n",
      "19 years\n",
      "20 .\n",
      "1 One\n",
      "2 of\n",
      "3 the\n",
      "4 hotels\n",
      "5 ,\n",
      "6 to\n",
      "7 be\n",
      "8 called\n",
      "9 the\n",
      "10 Sheraton\n",
      "11 Moscow\n",
      "12 ,\n",
      "13 will\n",
      "14 have\n",
      "15 450\n",
      "16 rooms\n",
      "17 and\n",
      "18 will\n",
      "19 cost\n",
      "20 an\n",
      "21 estimated\n",
      "22 $\n",
      "23 75\n",
      "24 million\n",
      "25 to\n",
      "26 build\n",
      "27 .\n",
      "1 The\n",
      "2 six-story\n",
      "3 hotel\n",
      "4 will\n",
      "5 be\n",
      "6 on\n",
      "7 Gorky\n",
      "8 Street\n",
      "9 and\n",
      "10 initially\n",
      "11 will\n",
      "12 cater\n",
      "13 mostly\n",
      "14 to\n",
      "15 business\n",
      "16 travelers\n",
      "17 .\n",
      "1 It\n",
      "2 will\n",
      "3 have\n",
      "4 a\n",
      "5 Russian\n",
      "6 tavern\n",
      "7 ,\n",
      "8 an\n",
      "9 English\n",
      "10 pub\n",
      "11 ,\n",
      "12 a\n",
      "13 discotheque\n",
      "14 and\n",
      "15 Japanese\n",
      "16 and\n",
      "17 Italian\n",
      "18 restaurants\n",
      "19 ,\n",
      "20 according\n",
      "21 to\n",
      "22 a\n",
      "23 Sheraton\n",
      "24 announcement\n",
      "25 .\n",
      "1 The\n",
      "2 hotel\n",
      "3 is\n",
      "4 scheduled\n",
      "5 to\n",
      "6 open\n",
      "7 in\n",
      "8 1992\n",
      "9 .\n",
      "1 The\n",
      "2 second\n",
      "3 hotel\n",
      "4 ,\n",
      "5 to\n",
      "6 be\n",
      "7 called\n",
      "8 the\n",
      "9 Budapest\n",
      "10 Hotel\n",
      "11 ,\n",
      "12 is\n",
      "13 to\n",
      "14 be\n",
      "15 constructed\n",
      "16 at\n",
      "17 a\n",
      "18 site\n",
      "19 even\n",
      "20 closer\n",
      "21 to\n",
      "22 Red\n",
      "23 Square\n",
      "24 .\n",
      "1 Details\n",
      "2 about\n",
      "3 its\n",
      "4 size\n",
      "5 and\n",
      "6 cost\n",
      "7 have\n",
      "8 n't\n",
      "9 yet\n",
      "10 been\n",
      "11 determined\n",
      "12 .\n",
      "1 Sheraton\n",
      "2 ,\n",
      "3 a\n",
      "4 subsidiary\n",
      "5 of\n",
      "6 ITT\n",
      "7 Corp.\n",
      "8 ,\n",
      "9 will\n",
      "10 have\n",
      "11 a\n",
      "12 40\n",
      "13 %\n",
      "14 share\n",
      "15 in"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  5%|▌         | 10/194 [01:58<36:18, 11.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "16 the\n",
      "17 two\n",
      "18 hotels\n",
      "19 ;\n",
      "20 Pan\n",
      "21 American\n",
      "22 ,\n",
      "23 a\n",
      "24 subsidiary\n",
      "25 of\n",
      "26 Pan\n",
      "27 Am\n",
      "28 Corp.\n",
      "29 ,\n",
      "30 will\n",
      "31 have\n",
      "32 a\n",
      "33 10\n",
      "34 %\n",
      "35 share\n",
      "36 .\n",
      "1 The\n",
      "2 Soviet\n",
      "3 owners\n",
      "4 will\n",
      "5 be\n",
      "6 Mossoviet\n",
      "7 ,\n",
      "8 Moscow\n",
      "9 's\n",
      "10 city\n",
      "11 governing\n",
      "12 body\n",
      "13 ,\n",
      "14 and\n",
      "15 Aeroflot\n",
      "16 ,\n",
      "17 the\n",
      "18 Soviet\n",
      "19 national\n",
      "20 airline\n",
      "21 .\n",
      "1 Although\n",
      "2 a\n",
      "3 Finnish\n",
      "4 group\n",
      "5 has\n",
      "6 a\n",
      "7 minority\n",
      "8 interest\n",
      "9 in\n",
      "10 an\n",
      "11 already\n",
      "12 operating\n",
      "13 Moscow\n",
      "14 hotel\n",
      "15 ,\n",
      "16 the\n",
      "17 Sheraton-Pan\n",
      "18 Am\n",
      "19 venture\n",
      "20 will\n",
      "21 be\n",
      "22 the\n",
      "23 first\n",
      "24 joint-venture\n",
      "25 hotels\n",
      "26 in\n",
      "27 the\n",
      "28 Soviet\n",
      "29 Union\n",
      "30 to\n",
      "31 have\n",
      "32 as\n",
      "33 much\n",
      "34 as\n",
      "35 50\n",
      "36 %\n",
      "37 foreign\n",
      "38 ownership\n",
      "39 .\n",
      "1 U.S.\n",
      "2 companies\n",
      "3 account\n",
      "4 for\n",
      "5 less\n",
      "6 than\n",
      "7 8\n",
      "8 %\n",
      "9 of\n",
      "10 the\n",
      "11 1,000\n",
      "12 or\n",
      "13 more\n",
      "14 Soviet\n",
      "15 joint\n",
      "16 ventures\n",
      "17 that\n",
      "18 have\n",
      "19 been\n",
      "20 announced\n",
      "21 since\n",
      "22 the\n",
      "23 Soviets\n",
      "24 began\n",
      "25 encouraging\n",
      "26 such\n",
      "27 undertakings\n",
      "28 in\n",
      "29 1987\n",
      "30 .\n",
      "1 But\n",
      "2 some\n",
      "3 U.S.\n",
      "4 companies\n",
      "5 are\n",
      "6 negotiating\n",
      "7 projects\n",
      "8 that\n",
      "9 could\n",
      "10 be\n",
      "11 among\n",
      "12 the\n",
      "13 biggest\n",
      "14 ones\n",
      "15 to\n",
      "16 be\n",
      "17 launched\n",
      "18 .\n",
      "1 Chevron\n",
      "2 Corp.\n",
      "3 ,\n",
      "4 Amoco\n",
      "5 Corp.\n",
      "6 ,\n",
      "7 Archer-Daniels-Midland\n",
      "8 Co.\n",
      "9 ,\n",
      "10 and\n",
      "11 Eastman\n",
      "12 Kodak\n",
      "13 Co.\n",
      "14 are\n",
      "15 among\n",
      "16 the\n",
      "17 U.S.\n",
      "18 companies\n",
      "19 known\n",
      "20 to\n",
      "21 be\n",
      "22 considering\n",
      "23 such\n",
      "24 ventures\n",
      "25 .\n",
      "1 Sheraton\n",
      "2 and\n",
      "3 Pan\n",
      "4 Am\n",
      "5 said\n",
      "6 they\n",
      "7 are\n",
      "8 assured\n",
      "9 under\n",
      "10 the\n",
      "11 Soviet\n",
      "12 joint-venture\n",
      "13 law\n",
      "14 that\n",
      "15 they\n",
      "16 can\n",
      "17 repatriate\n",
      "18 profits\n",
      "19 from\n",
      "20 their\n",
      "21 hotel\n",
      "22 venture\n",
      "23 .\n",
      "1 The\n",
      "2 Sheraton\n",
      "3 Moscow\n",
      "4 will\n",
      "5 charge\n",
      "6 about\n",
      "7 $\n",
      "8 140\n",
      "9 to\n",
      "10 $\n",
      "11 150\n",
      "12 a\n",
      "13 day\n",
      "14 for\n",
      "15 each\n",
      "16 of\n",
      "17 its\n",
      "18 rooms\n",
      "19 ,\n",
      "20 and\n",
      "21 it\n",
      "22 will\n",
      "23 accept\n",
      "24 payment\n",
      "25 only\n",
      "26 in\n",
      "27 currencies\n",
      "28 that\n",
      "29 can\n",
      "30 be\n",
      "31 traded\n",
      "32 in\n",
      "33 foreign\n",
      "34 exchange\n",
      "35 markets\n",
      "36 ,\n",
      "37 according\n",
      "38 to\n",
      "39 a\n",
      "40 Sheraton\n",
      "41 executive\n",
      "42 .\n",
      "1 Thomas\n",
      "2 Plaskett\n",
      "3 ,\n",
      "4 Pan\n",
      "5 Am\n",
      "6 's\n",
      "7 chairman\n",
      "8 ,\n",
      "9 said\n",
      "10 the\n",
      "11 U.S.\n",
      "12 airline\n",
      "13 's\n",
      "14 participation\n",
      "15 is\n",
      "16 a\n",
      "17 natural\n",
      "18 outgrowth\n",
      "19 of\n",
      "20 its\n",
      "21 current\n",
      "22 arrangements\n",
      "23 with\n",
      "24 Aeroflot\n",
      "25 to\n",
      "26 jointly\n",
      "27 operate\n",
      "28 nonstop\n",
      "29 New\n",
      "30 York-Moscow\n",
      "31 flights\n",
      "32 .\n",
      "1 He\n",
      "2 said\n",
      "3 the\n",
      "4 rising\n",
      "5 volume\n",
      "6 of\n",
      "7 passenger\n",
      "8 traffic\n",
      "9 on\n",
      "10 this\n",
      "11 route\n",
      "12 justifies\n",
      "13 a\n",
      "14 major\n",
      "15 investment\n",
      "16 in\n",
      "17 new\n",
      "18 high-standard\n",
      "19 Moscow\n",
      "20 hotels\n",
      "21 .\n",
      "1 The\n",
      "2 government\n",
      "3 sold\n",
      "4 the\n",
      "5 deposits\n",
      "6 of\n",
      "7 four\n",
      "8 savings-and-loan\n",
      "9 institutions\n",
      "10 ,\n",
      "11 in\n",
      "12 its\n",
      "13 first\n",
      "14 wave\n",
      "15 of\n",
      "16 sales\n",
      "17 of\n",
      "18 big\n",
      "19 ,\n",
      "20 sick\n",
      "21 thrifts\n",
      "22 ,\n",
      "23 but\n",
      "24 low\n",
      "25 bids\n",
      "26 prevented\n",
      "27 the\n",
      "28 sale\n",
      "29 of\n",
      "30 a\n",
      "31 fifth\n",
      "32 .\n",
      "1 The\n",
      "2 four\n",
      "3 S\n",
      "4 &\n",
      "5 Ls\n",
      "6 were\n",
      "7 sold\n",
      "8 to\n",
      "9 large\n",
      "10 banks\n",
      "11 ,\n",
      "12 as\n",
      "13 was\n",
      "14 the\n",
      "15 case\n",
      "16 with\n",
      "17 most\n",
      "18 of\n",
      "19 the\n",
      "20 28\n",
      "21 previous\n",
      "22 transactions\n",
      "23 initiated\n",
      "24 by\n",
      "25 the\n",
      "26 Resolution\n",
      "27 Trust\n",
      "28 Corp\n",
      "29 .\n",
      "1 since\n",
      "2 it\n",
      "3 was\n",
      "4 created\n",
      "5 in\n",
      "6 the\n",
      "7 S\n",
      "8 &\n",
      "9 L\n",
      "10 bailout\n",
      "11 legislation\n",
      "12 two\n",
      "13 months\n",
      "14 ago\n",
      "15 .\n",
      "1 Two\n",
      "2 of\n",
      "3 the\n",
      "4 four\n",
      "5 big\n",
      "6 thrifts\n",
      "7 were\n",
      "8 sold\n",
      "9 to\n",
      "10 NCNB\n",
      "11 Corp.\n",
      "12 ,\n",
      "13 Charlotte\n",
      "14 ,\n",
      "15 N.C.\n",
      "16 ,\n",
      "17 which\n",
      "18 has\n",
      "19 aggressively\n",
      "20 expanded\n",
      "21 its\n",
      "22 markets\n",
      "23 ,\n",
      "24 particularly\n",
      "25 in\n",
      "26 Texas\n",
      "27 and\n",
      "28 Florida\n",
      "29 .\n",
      "1 A\n",
      "2 Canadian\n",
      "3 bank\n",
      "4 bought\n",
      "5 another\n",
      "6 thrift\n",
      "7 ,\n",
      "8 in\n",
      "9 the\n",
      "10 first\n",
      "11 RTC\n",
      "12 transaction\n",
      "13 with\n",
      "14 a\n",
      "15 foreign\n",
      "16 bank\n",
      "17 .\n",
      "1 Under\n",
      "2 these\n",
      "3 deals\n",
      "4 ,\n",
      "5 the\n",
      "6 RTC\n",
      "7 sells\n",
      "8 just\n",
      "9 the\n",
      "10 deposits\n",
      "11 and\n",
      "12 the\n",
      "13 healthy\n",
      "14 assets\n",
      "15 .\n",
      "1 These\n",
      "2 ``\n",
      "3 clean-bank\n",
      "4 ''\n",
      "5 transactions\n",
      "6 leave\n",
      "7 the\n",
      "8 bulk\n",
      "9 of\n",
      "10 bad\n",
      "11 assets\n",
      "12 ,\n",
      "13 mostly\n",
      "14 real\n",
      "15 estate\n",
      "16 ,\n",
      "17 with\n",
      "18 the\n",
      "19 government\n",
      "20 ,\n",
      "21 to\n",
      "22 be\n",
      "23 sold\n",
      "24 later\n",
      "25 .\n",
      "1 In\n",
      "2 these\n",
      "3 four\n",
      "4 ,\n",
      "5 for\n",
      "6 instance\n",
      "7 ,\n",
      "8 the\n",
      "9 RTC\n",
      "10 is\n",
      "11 stuck\n",
      "12 with\n",
      "13 $\n",
      "14 4.51\n",
      "15 billion\n",
      "16 in\n",
      "17 bad\n",
      "18 assets\n",
      "19 .\n",
      "1 Acquirers\n",
      "2 paid\n",
      "3 premiums\n",
      "4 ranging\n",
      "5 from\n",
      "6 1.5\n",
      "7 %\n",
      "8 to\n",
      "9 3.7\n",
      "10 %\n",
      "11 for\n",
      "12 the\n",
      "13 deposits\n",
      "14 and\n",
      "15 branch\n",
      "16 systems\n",
      "17 ,\n",
      "18 roughly\n",
      "19 in\n",
      "20 line\n",
      "21 with\n",
      "22 what\n",
      "23 analysts\n",
      "24 were\n",
      "25 expecting\n",
      "26 .\n",
      "1 The\n",
      "2 buyers\n",
      "3 will\n",
      "4 also\n",
      "5 be\n",
      "6 locked\n",
      "7 into\n",
      "8 deposit\n",
      "9 rates\n",
      "10 for\n",
      "11 just\n",
      "12 two\n",
      "13 weeks\n",
      "14 ,\n",
      "15 as\n",
      "16 has\n",
      "17 been\n",
      "18 the\n",
      "19 case\n",
      "20 with\n",
      "21 previous\n",
      "22 deals\n",
      "23 .\n",
      "1 After\n",
      "2 that\n",
      "3 ,\n",
      "4 the\n",
      "5 buyers\n",
      "6 may\n",
      "7 repudiate\n",
      "8"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  6%|▌         | 11/194 [02:23<39:55, 13.09s/it]"
     ]
    }
   ],
   "source": [
    "for file_name in tqdm(bad_file_names, total=len(bad_file_names)):\n",
    "    if file_name in train_file_names:\n",
    "        extract_features(file_name, model, \"TRAINING/\")\n",
    "    else:\n",
    "        extract_features(file_name, model, \"TEST/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
