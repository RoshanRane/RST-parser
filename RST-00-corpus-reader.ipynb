{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Downloading tqdm-4.19.5-py2.py3-none-any.whl (51kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 1.9MB/s ta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: tqdm\n",
      "Successfully installed tqdm-4.19.5\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import gensim\n",
    "import pandas\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# constructing the tree: string to sequence of its elements\n",
    "def split(tree_str):\n",
    "    chunks = tree_str.split()\n",
    "    chunk_elements = []\n",
    "    for chunk in chunks:\n",
    "        if chunk[0] == \"(\":\n",
    "            index_i = 0\n",
    "            while chunk[index_i] == \"(\":\n",
    "                chunk_elements.append(\"(\")\n",
    "                index_i += 1\n",
    "            chunk_elements.append(chunk[index_i:])\n",
    "        else:\n",
    "            index_j = (-1)\n",
    "            while chunk[index_j] == \")\":\n",
    "                chunk_elements.append(\")\")\n",
    "                index_j += (-1)\n",
    "            chunk_elements.insert((index_j + 1), chunk[0:((index_j)+1)])\n",
    "    return(chunk_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generating constituents\n",
    "def constituents(tree_str):\n",
    "# nt = non-terminal\n",
    "    nt_seq = []\n",
    "    term_counter = 0\n",
    "    constituents_set = set()\n",
    "    parsed_str = split(tree_str)\n",
    "    element_index = 0\n",
    "    none_depth = 0\n",
    "    depth = 0\n",
    "    while element_index < len(parsed_str):\n",
    "        if parsed_str[element_index] == \"(\":\n",
    "            nt = parsed_str[element_index + 1]\n",
    "            depth += 1\n",
    "            if nt == \"-NONE-\":\n",
    "# we are entering a subtree beginning with -NONE-                \n",
    "                none_depth += 1\n",
    "                nt_seq.append((nt, term_counter + 1))\n",
    "            else:\n",
    "# taking first of the nt-label\n",
    "                if len(nt) > 0 and nt[0] == \"-\":\n",
    "                    first_nt_label = nt\n",
    "                else:\n",
    "                    first_nt_label = nt.split(\"-\")[0]\n",
    "                nt_seq.append((first_nt_label, term_counter + 1))\n",
    "#  skipping a non-terminal           \n",
    "            element_index += 2\n",
    "        elif parsed_str[element_index] == \")\":\n",
    "            last_nt = nt_seq.pop()\n",
    "            depth -= 1\n",
    "            if last_nt[0] == \"-NONE-\":\n",
    "# exiting a subtree beginning with -NONE-                \n",
    "                none_depth -= 1\n",
    "            else:\n",
    "                if last_nt[1] <= term_counter:\n",
    "                    if last_nt[0] == \"ROOT\" or last_nt[0] ==\"TOP\":\n",
    "                        pass\n",
    "                    else:\n",
    "                        constituents_set.add((last_nt[0], last_nt[1], term_counter, depth))\n",
    "                else:\n",
    "# In this case all terminals in the subtree were ignored, which means that the subtree contained nothing but traces, \n",
    "# therefore we're skipping the constituent.\n",
    "                    pass\n",
    "            element_index += 1\n",
    "        else:\n",
    "            if none_depth == 0:\n",
    "# if we are not in a subtree beginning with -NONE-, then we count the terminal                \n",
    "                term_counter += 1\n",
    "            else:\n",
    "# otherwise we ignore the terminal            \n",
    "                pass\n",
    "            element_index += 1\n",
    "    return constituents_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(file_name, word2vec_model, subfolder):\n",
    "    # Reading the EDU file\n",
    "    RST_file = open(os.path.join(\"../nlp_project/rst_discourse_treebank/data/RSTtrees-WSJ-main-1.0/\" + subfolder,\n",
    "                                 file_name) + \".edus\")\n",
    "    lines = RST_file.readlines()\n",
    "    RST_file.close()\n",
    "    \n",
    "    # tokenizing the EDUs\n",
    "    tokenized_edus = []\n",
    "    for edu in lines:\n",
    "        tokenized_edu = []\n",
    "        for token in word_tokenize(edu):\n",
    "            if token == \"(\":\n",
    "                tokenized_edu.append(\"OPENING_ROUND_BRACE\")\n",
    "            elif token == \")\":\n",
    "                tokenized_edu.append(\"CLOSING_ROUND_BRACE\")\n",
    "            else:\n",
    "                tokenized_edu.append(token)\n",
    "        tokenized_edus.append(tokenized_edu)\n",
    "        \n",
    "    # Remember boundary indices, combine EDUs of a sentence\n",
    "    boundary_indices = [] \n",
    "    edu_boundary = []\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    for edu in tokenized_edus:\n",
    "        if (edu[-1] not in [\"!\", \"?\", \".\"])\\\n",
    "                and (edu[-2:] not in [\"!\\\"\", \"?\\\"\", \".\\\"\", \"!'\", \"?'\", \".'\"])\\\n",
    "                and (edu[-3:] not in [\"!''\", \"?''\", \".''\"]):\n",
    "            sentence.extend(edu)\n",
    "            edu_boundary.append(len(sentence) - 1)\n",
    "        else:\n",
    "            sentence.extend(edu)\n",
    "            sentences.append(sentence)\n",
    "            edu_boundary.append(len(sentence) - 1)\n",
    "            boundary_indices.append(edu_boundary)\n",
    "            sentence = []\n",
    "            edu_boundary = []\n",
    "            \n",
    "    # getting a POS-tag from nltk\n",
    "    pos_sentences = []\n",
    "    for sentence in sentences:\n",
    "        pos_sentence = nltk.pos_tag(sentence)\n",
    "        pos_sentences.append(pos_sentence)\n",
    "        \n",
    "    # writing a text in conll format\n",
    "#     conll_file = open(\"edu_segmentation/\" + file_name + \".conll\", \"w\")\n",
    "    \n",
    "#     for pos_sentence in pos_sentences:\n",
    "#         for word, pos in pos_sentence:\n",
    "#             conll_file.write(word + \" \" + pos + \"\\n\")\n",
    "#         conll_file.write(\"\\n\")\n",
    "\n",
    "#     conll_file.close()\n",
    "    \n",
    "    # writing the text in a file for syntactic parsing\n",
    "    text_file = open(\"edu_segmentation/\" + file_name + \".text\", \"w\")\n",
    "    text_file.write(\"\\n\".join([\" \".join(sentence) for sentence in sentences]))\n",
    "    text_file.close()\n",
    "\n",
    "    os.environ[\"FILE_NAME\"] = file_name\n",
    "\n",
    "    # performing syntactic parsing\n",
    "    os.system(\"./stanford-parser-full-2017-06-09/lexparser.sh edu_segmentation/\\\"$FILE_NAME\\\".text > edu_segmentation/\\\"$FILE_NAME\\\".penn\")\n",
    "\n",
    "    with open(\"edu_segmentation/\" + file_name + \".penn\") as file:\n",
    "        trees_string = file.read()\n",
    "        trees = trees_string.split(\"\\n\\n\")\n",
    "    \n",
    "    one_word_constituents = []\n",
    "    for tree in trees:\n",
    "        tree_dict = {}\n",
    "        for constituent in constituents(tree):\n",
    "            if constituent[1] not in tree_dict:\n",
    "                tree_dict[constituent[1]] = ([],[])\n",
    "            tree_dict[constituent[1]][0].append(constituent)\n",
    "            if constituent[2] not in tree_dict:\n",
    "                tree_dict[constituent[2]] = ([],[])\n",
    "            second_constituent = tree_dict[constituent[2]][1].append(constituent)\n",
    "        one_word_constituents.append(tree_dict)\n",
    "        \n",
    "    # next we will compute the top syntactic tag\n",
    "    top_tags = []\n",
    "    # create a new list (it will contain dictionaries with top syntactic tags)\n",
    "    for tree_dict in one_word_constituents:\n",
    "        tags_dict = {}\n",
    "        for key, value in tree_dict.items(): # e.g. 1 --> key, ([(\"NP\", 1, 6), (\"S\", 1, 44)], [(\"NNP\", 1, 1)]) -->value\n",
    "            top_tag_begin = max([(element[2], element[0]) for element in value[0]])[1]\n",
    "            top_tag_end = min([(element[1], element[0]) for element in value[1]])[1]\n",
    "\n",
    "            tags_dict.update({key: (top_tag_begin, top_tag_end)})\n",
    "        top_tags.append(tags_dict)\n",
    "    \n",
    "#     Adding the Depth to the list with Top Syntactic Tags\n",
    "    top_depths = []\n",
    "    for tree_dict in one_word_constituents:\n",
    "        tags_dict = {}\n",
    "        for key, value in tree_dict.items():\n",
    "            top_tag_begin_depth = max([(element[2], element[3]) for element in value[0]])[1]\n",
    "            top_tag_end_depth = min([(element[1], element[3]) for element in value[1]])[1]\n",
    "\n",
    "            tags_dict.update({key: (top_tag_begin_depth, top_tag_end_depth)})\n",
    "        top_depths.append(tags_dict)\n",
    "    \n",
    "    #enumerate tokens of the sentence (use *enumerate*) and iterate over them \n",
    "    # (the current token will be the middle left 4-gram element).\n",
    "    data = []\n",
    "    #check if there is a edu break after the token\n",
    "    for (sentence_no, pos_sentence) in enumerate(pos_sentences):\n",
    "        for (token_no, token) in enumerate(pos_sentence): #token_no = key\n",
    "            if token in boundary_indices[sentence_no]: # taking boundary indices of a corresponding sentence\n",
    "                edu_break = True\n",
    "            else:\n",
    "                edu_break = False\n",
    "\n",
    "    # take a top_tag_beg from top_tags and add it to the table with sent_no\n",
    "            top_tag_beg = top_tags[sentence_no][token_no +1][0] #because we've counted them starting from 1\n",
    "            top_tag_end = top_tags[sentence_no][token_no +1][1]\n",
    "            depth_beg = top_depths[sentence_no][token_no +1][0]\n",
    "            depth_end = top_depths[sentence_no][token_no +1][1] \n",
    "\n",
    "    # get a vector\n",
    "            word = token[0]\n",
    "            POS_tag = token[1]\n",
    "\n",
    "    # form a tuple with features which i'll put to the data\n",
    "\n",
    "            feature_vectors = []\n",
    "\n",
    "            feature_vectors.append(file_name)\n",
    "            feature_vectors.append(sentence_no)\n",
    "            feature_vectors.append(token_no)\n",
    "            feature_vectors.append(word)\n",
    "            feature_vectors.append(edu_break)\n",
    "            feature_vectors.append(POS_tag)\n",
    "            feature_vectors.append(top_tag_beg)\n",
    "            feature_vectors.append(top_tag_end)\n",
    "            feature_vectors.append(depth_beg)\n",
    "            feature_vectors.append(depth_end)\n",
    "\n",
    "            if word in model.vocab:\n",
    "                word_vector = model.get_vector(word)\n",
    "                feature_vectors.extend(list(word_vector))\n",
    "            else:\n",
    "                feature_vectors.extend([0]*300)\n",
    "\n",
    "            data.append(feature_vectors)\n",
    "            \n",
    "    df = pd.DataFrame(data, columns=[\"file_name\", \n",
    "                                 \"sentence_no\", \n",
    "                                 \"token_no\", \n",
    "                                 \"word\", \n",
    "                                 \"edu_break\", \n",
    "                                 \"POS-tag\", \n",
    "                                 \"top_tag_beg\", \n",
    "                                 \"top_tag_end\", \n",
    "                                 \"depth_beg\", \n",
    "                                 \"depth_end\"] + [\"c_%.3d\" % i for i in range(300)])\n",
    "    \n",
    "    df.to_csv(\"edu_segmentation/\" + file_name + \".csv\", index=False)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Google's pre-trained Word2Vec model.\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_file_names(ls_result, target_ext):\n",
    "    file_names = []\n",
    "    \n",
    "    for fn in ls_result:\n",
    "        ext = fn[-4:]\n",
    "        if ext == target_ext:\n",
    "            file_names.append(fn[:-4])\n",
    "                \n",
    "    return file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_features(subfolder, file_names):\n",
    "    for file_name in tqdm(file_names, total=len(file_names)):\n",
    "        extract_features(file_name, model, subfolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_csv(subfolder):\n",
    "    ls_result = os.listdir(\"rst_discourse_treebank/data/RSTtrees-WSJ-main-1.0/\" + subfolder)\n",
    "    file_names = get_file_names(ls_result)\n",
    "    compute_features(subfolder, file_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ls_result = os.listdir(\"edu_segmentation/\")\n",
    "csv_file_names = get_file_names(ls_result, \".csv\")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "dfs = []\n",
    "for file in csv_file_names:\n",
    "    df = pd.read_csv(\"edu_segmentation/\" + file + \".csv\")\n",
    "    dfs.append(df)\n",
    "\n",
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_table = pd.get_dummies(df, columns=[\"token_no\", \"POS-tag\", \"top_tag_beg\", \"top_tag_end\", \"depth_beg\", \"depth_end\",])\n",
    "vectorized_table.to_csv(\"data_set.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
